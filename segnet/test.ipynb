{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [01:44<00:00, 104.31s/it]\n"
     ]
    }
   ],
   "source": [
    "%run gen_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 256, 256)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 256, 256)      1024      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 256, 256)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 256, 256)      1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 128, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 256, 64, 64)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 256, 64, 64)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 256, 64, 64)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 32, 32)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 512, 32, 32)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 512, 32, 32)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 512, 16, 16)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512, 16, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 512, 16, 16)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512, 16, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 512, 16, 16)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 512, 16, 16)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 512, 16, 16)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512, 16, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 512, 16, 16)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512, 16, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 512, 16, 16)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 512, 16, 16)       64        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 512, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 512, 32, 32)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 512, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 512, 32, 32)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 512, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 512, 32, 32)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 512, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 512, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 256, 64, 64)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 256, 64, 64)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 256, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 256, 64, 64)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 256, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 256, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 128, 128, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 128, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 64, 256, 256)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 64, 256, 256)      1024      \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 64, 256, 256)      36928     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 64, 256, 256)      1024      \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 2, 256, 256)       130       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 65536)          0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 65536, 2)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 65536, 2)          0         \n",
      "=================================================================\n",
      "Total params: 31,795,906\n",
      "Trainable params: 31,791,490\n",
      "Non-trainable params: 4,416\n",
      "_________________________________________________________________\n",
      "the number of train data is 256\n",
      "the number of val data is 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\OneDrive\\Satellite-Segmentation\\segnet\\segnet_train-gf.py:232: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=32, epochs=32, verbose=1, validation_data=<generator..., validation_steps=32, callbacks=[<keras.ca..., max_queue_size=1)`\n",
      "  validation_data=generateValidData(BS,val_set),validation_steps=valid_numb//BS,callbacks=callable,max_q_size=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "32/32 [==============================] - ETA: 2:35 - loss: 0.9801 - acc: 0.521 - ETA: 1:23 - loss: 0.8426 - acc: 0.572 - ETA: 58s - loss: 0.7539 - acc: 0.570 - ETA: 45s - loss: 0.8008 - acc: 0.59 - ETA: 38s - loss: 0.7643 - acc: 0.61 - ETA: 32s - loss: 0.7754 - acc: 0.61 - ETA: 28s - loss: 0.7377 - acc: 0.63 - ETA: 25s - loss: 0.7273 - acc: 0.62 - ETA: 23s - loss: 0.7021 - acc: 0.64 - ETA: 21s - loss: 0.6733 - acc: 0.66 - ETA: 19s - loss: 0.6580 - acc: 0.66 - ETA: 17s - loss: 0.6486 - acc: 0.67 - ETA: 16s - loss: 0.6639 - acc: 0.66 - ETA: 14s - loss: 0.6629 - acc: 0.65 - ETA: 13s - loss: 0.6658 - acc: 0.66 - ETA: 12s - loss: 0.6584 - acc: 0.66 - ETA: 11s - loss: 0.6530 - acc: 0.66 - ETA: 10s - loss: 0.6537 - acc: 0.65 - ETA: 9s - loss: 0.6712 - acc: 0.6540 - ETA: 8s - loss: 0.6592 - acc: 0.660 - ETA: 7s - loss: 0.6598 - acc: 0.664 - ETA: 7s - loss: 0.6616 - acc: 0.666 - ETA: 6s - loss: 0.6603 - acc: 0.670 - ETA: 5s - loss: 0.6510 - acc: 0.675 - ETA: 4s - loss: 0.6496 - acc: 0.676 - ETA: 4s - loss: 0.6485 - acc: 0.677 - ETA: 3s - loss: 0.6443 - acc: 0.680 - ETA: 2s - loss: 0.6337 - acc: 0.687 - ETA: 1s - loss: 0.6309 - acc: 0.692 - ETA: 1s - loss: 0.6244 - acc: 0.696 - ETA: 0s - loss: 0.6178 - acc: 0.699 - 27s 850ms/step - loss: 0.6156 - acc: 0.6988 - val_loss: 1.4279 - val_acc: 0.7037\n",
      "Epoch 2/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.3298 - acc: 0.86 - ETA: 15s - loss: 0.4537 - acc: 0.71 - ETA: 14s - loss: 0.4325 - acc: 0.72 - ETA: 14s - loss: 0.5137 - acc: 0.71 - ETA: 13s - loss: 0.5071 - acc: 0.72 - ETA: 13s - loss: 0.5219 - acc: 0.70 - ETA: 12s - loss: 0.5114 - acc: 0.72 - ETA: 12s - loss: 0.5134 - acc: 0.72 - ETA: 11s - loss: 0.5078 - acc: 0.72 - ETA: 11s - loss: 0.4891 - acc: 0.73 - ETA: 10s - loss: 0.4842 - acc: 0.73 - ETA: 10s - loss: 0.4852 - acc: 0.73 - ETA: 9s - loss: 0.5060 - acc: 0.7337 - ETA: 9s - loss: 0.5314 - acc: 0.719 - ETA: 8s - loss: 0.5324 - acc: 0.717 - ETA: 8s - loss: 0.5280 - acc: 0.720 - ETA: 7s - loss: 0.5253 - acc: 0.724 - ETA: 7s - loss: 0.5309 - acc: 0.724 - ETA: 6s - loss: 0.5534 - acc: 0.715 - ETA: 6s - loss: 0.5501 - acc: 0.718 - ETA: 5s - loss: 0.5477 - acc: 0.720 - ETA: 5s - loss: 0.5543 - acc: 0.719 - ETA: 4s - loss: 0.5585 - acc: 0.723 - ETA: 4s - loss: 0.5558 - acc: 0.727 - ETA: 3s - loss: 0.5579 - acc: 0.726 - ETA: 3s - loss: 0.5617 - acc: 0.726 - ETA: 2s - loss: 0.5625 - acc: 0.728 - ETA: 2s - loss: 0.5581 - acc: 0.733 - ETA: 1s - loss: 0.5554 - acc: 0.737 - ETA: 1s - loss: 0.5524 - acc: 0.739 - ETA: 0s - loss: 0.5496 - acc: 0.741 - 23s 714ms/step - loss: 0.5513 - acc: 0.7390 - val_loss: 0.7500 - val_acc: 0.7041\n",
      "Epoch 3/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.4117 - acc: 0.86 - ETA: 15s - loss: 0.5344 - acc: 0.71 - ETA: 14s - loss: 0.4664 - acc: 0.77 - ETA: 14s - loss: 0.5293 - acc: 0.75 - ETA: 13s - loss: 0.5173 - acc: 0.75 - ETA: 13s - loss: 0.5402 - acc: 0.73 - ETA: 12s - loss: 0.5307 - acc: 0.74 - ETA: 12s - loss: 0.5391 - acc: 0.72 - ETA: 11s - loss: 0.5337 - acc: 0.73 - ETA: 11s - loss: 0.5179 - acc: 0.74 - ETA: 10s - loss: 0.5162 - acc: 0.74 - ETA: 10s - loss: 0.5157 - acc: 0.74 - ETA: 9s - loss: 0.5421 - acc: 0.7259 - ETA: 9s - loss: 0.5388 - acc: 0.728 - ETA: 8s - loss: 0.5380 - acc: 0.727 - ETA: 8s - loss: 0.5364 - acc: 0.723 - ETA: 7s - loss: 0.5351 - acc: 0.723 - ETA: 7s - loss: 0.5407 - acc: 0.715 - ETA: 6s - loss: 0.5372 - acc: 0.721 - ETA: 6s - loss: 0.5324 - acc: 0.724 - ETA: 5s - loss: 0.5339 - acc: 0.725 - ETA: 5s - loss: 0.5349 - acc: 0.724 - ETA: 4s - loss: 0.5354 - acc: 0.729 - ETA: 4s - loss: 0.5340 - acc: 0.732 - ETA: 3s - loss: 0.5347 - acc: 0.731 - ETA: 3s - loss: 0.5381 - acc: 0.731 - ETA: 2s - loss: 0.5374 - acc: 0.732 - ETA: 2s - loss: 0.5317 - acc: 0.737 - ETA: 1s - loss: 0.5282 - acc: 0.741 - ETA: 1s - loss: 0.5246 - acc: 0.743 - ETA: 0s - loss: 0.5201 - acc: 0.745 - 23s 715ms/step - loss: 0.5187 - acc: 0.7428 - val_loss: 0.5428 - val_acc: 0.7630\n",
      "Epoch 4/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.2807 - acc: 0.92 - ETA: 15s - loss: 0.4454 - acc: 0.74 - ETA: 14s - loss: 0.4417 - acc: 0.71 - ETA: 14s - loss: 0.4872 - acc: 0.70 - ETA: 13s - loss: 0.4780 - acc: 0.72 - ETA: 13s - loss: 0.4907 - acc: 0.70 - ETA: 12s - loss: 0.4844 - acc: 0.71 - ETA: 12s - loss: 0.4885 - acc: 0.70 - ETA: 11s - loss: 0.4849 - acc: 0.70 - ETA: 11s - loss: 0.4676 - acc: 0.72 - ETA: 10s - loss: 0.4713 - acc: 0.72 - ETA: 10s - loss: 0.4721 - acc: 0.71 - ETA: 9s - loss: 0.4961 - acc: 0.7085 - ETA: 9s - loss: 0.4937 - acc: 0.712 - ETA: 8s - loss: 0.4935 - acc: 0.712 - ETA: 8s - loss: 0.4894 - acc: 0.716 - ETA: 7s - loss: 0.4859 - acc: 0.720 - ETA: 7s - loss: 0.4866 - acc: 0.721 - ETA: 6s - loss: 0.4874 - acc: 0.718 - ETA: 6s - loss: 0.4858 - acc: 0.718 - ETA: 5s - loss: 0.4861 - acc: 0.715 - ETA: 5s - loss: 0.4866 - acc: 0.715 - ETA: 4s - loss: 0.4880 - acc: 0.717 - ETA: 4s - loss: 0.4830 - acc: 0.721 - ETA: 3s - loss: 0.4831 - acc: 0.720 - ETA: 3s - loss: 0.4837 - acc: 0.722 - ETA: 2s - loss: 0.4850 - acc: 0.721 - ETA: 2s - loss: 0.4804 - acc: 0.727 - ETA: 1s - loss: 0.4788 - acc: 0.729 - ETA: 1s - loss: 0.4756 - acc: 0.732 - ETA: 0s - loss: 0.4708 - acc: 0.734 - 23s 719ms/step - loss: 0.4694 - acc: 0.7329 - val_loss: 0.4634 - val_acc: 0.7729\n",
      "Epoch 5/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2363 - acc: 0.91 - ETA: 15s - loss: 0.3410 - acc: 0.85 - ETA: 15s - loss: 0.3957 - acc: 0.79 - ETA: 14s - loss: 0.4420 - acc: 0.76 - ETA: 14s - loss: 0.4319 - acc: 0.76 - ETA: 13s - loss: 0.4434 - acc: 0.75 - ETA: 13s - loss: 0.4326 - acc: 0.76 - ETA: 12s - loss: 0.4392 - acc: 0.75 - ETA: 12s - loss: 0.4439 - acc: 0.75 - ETA: 11s - loss: 0.4276 - acc: 0.76 - ETA: 10s - loss: 0.4237 - acc: 0.76 - ETA: 10s - loss: 0.4294 - acc: 0.75 - ETA: 9s - loss: 0.4532 - acc: 0.7421 - ETA: 9s - loss: 0.4500 - acc: 0.741 - ETA: 8s - loss: 0.4499 - acc: 0.738 - ETA: 8s - loss: 0.4465 - acc: 0.744 - ETA: 7s - loss: 0.4433 - acc: 0.748 - ETA: 7s - loss: 0.4457 - acc: 0.750 - ETA: 6s - loss: 0.4476 - acc: 0.744 - ETA: 6s - loss: 0.4478 - acc: 0.743 - ETA: 5s - loss: 0.4491 - acc: 0.742 - ETA: 5s - loss: 0.4506 - acc: 0.742 - ETA: 4s - loss: 0.4532 - acc: 0.742 - ETA: 4s - loss: 0.4491 - acc: 0.745 - ETA: 3s - loss: 0.4491 - acc: 0.744 - ETA: 3s - loss: 0.4502 - acc: 0.745 - ETA: 2s - loss: 0.4506 - acc: 0.743 - ETA: 2s - loss: 0.4457 - acc: 0.748 - ETA: 1s - loss: 0.4451 - acc: 0.748 - ETA: 1s - loss: 0.4427 - acc: 0.749 - ETA: 0s - loss: 0.4382 - acc: 0.752 - 23s 718ms/step - loss: 0.4374 - acc: 0.7515 - val_loss: 0.4294 - val_acc: 0.7918\n",
      "Epoch 6/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.2148 - acc: 0.91 - ETA: 15s - loss: 0.3233 - acc: 0.86 - ETA: 15s - loss: 0.3680 - acc: 0.80 - ETA: 15s - loss: 0.4124 - acc: 0.77 - ETA: 14s - loss: 0.4004 - acc: 0.77 - ETA: 13s - loss: 0.4203 - acc: 0.75 - ETA: 13s - loss: 0.4101 - acc: 0.76 - ETA: 12s - loss: 0.4187 - acc: 0.75 - ETA: 12s - loss: 0.4170 - acc: 0.75 - ETA: 11s - loss: 0.4036 - acc: 0.76 - ETA: 11s - loss: 0.3998 - acc: 0.76 - ETA: 10s - loss: 0.4063 - acc: 0.76 - ETA: 10s - loss: 0.4289 - acc: 0.76 - ETA: 9s - loss: 0.4265 - acc: 0.7612 - ETA: 9s - loss: 0.4276 - acc: 0.758 - ETA: 8s - loss: 0.4232 - acc: 0.765 - ETA: 7s - loss: 0.4198 - acc: 0.768 - ETA: 7s - loss: 0.4214 - acc: 0.769 - ETA: 6s - loss: 0.4235 - acc: 0.764 - ETA: 6s - loss: 0.4231 - acc: 0.763 - ETA: 5s - loss: 0.4273 - acc: 0.759 - ETA: 5s - loss: 0.4291 - acc: 0.758 - ETA: 4s - loss: 0.4310 - acc: 0.759 - ETA: 4s - loss: 0.4282 - acc: 0.763 - ETA: 3s - loss: 0.4291 - acc: 0.762 - ETA: 3s - loss: 0.4301 - acc: 0.762 - ETA: 2s - loss: 0.4308 - acc: 0.760 - ETA: 2s - loss: 0.4267 - acc: 0.765 - ETA: 1s - loss: 0.4267 - acc: 0.764 - ETA: 1s - loss: 0.4235 - acc: 0.765 - ETA: 0s - loss: 0.4187 - acc: 0.769 - 23s 724ms/step - loss: 0.4180 - acc: 0.7715 - val_loss: 0.4247 - val_acc: 0.7954\n",
      "Epoch 7/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 15s - loss: 0.2098 - acc: 0.92 - ETA: 15s - loss: 0.2877 - acc: 0.89 - ETA: 15s - loss: 0.3328 - acc: 0.83 - ETA: 14s - loss: 0.3927 - acc: 0.80 - ETA: 14s - loss: 0.3826 - acc: 0.80 - ETA: 13s - loss: 0.4114 - acc: 0.79 - ETA: 13s - loss: 0.4033 - acc: 0.79 - ETA: 12s - loss: 0.4115 - acc: 0.79 - ETA: 12s - loss: 0.4126 - acc: 0.78 - ETA: 11s - loss: 0.4000 - acc: 0.79 - ETA: 11s - loss: 0.3959 - acc: 0.79 - ETA: 10s - loss: 0.4035 - acc: 0.78 - ETA: 9s - loss: 0.4247 - acc: 0.7853 - ETA: 9s - loss: 0.4219 - acc: 0.782 - ETA: 8s - loss: 0.4230 - acc: 0.778 - ETA: 8s - loss: 0.4185 - acc: 0.784 - ETA: 7s - loss: 0.4153 - acc: 0.786 - ETA: 7s - loss: 0.4172 - acc: 0.785 - ETA: 6s - loss: 0.4187 - acc: 0.781 - ETA: 6s - loss: 0.4185 - acc: 0.779 - ETA: 5s - loss: 0.4216 - acc: 0.775 - ETA: 5s - loss: 0.4228 - acc: 0.776 - ETA: 4s - loss: 0.4248 - acc: 0.775 - ETA: 4s - loss: 0.4213 - acc: 0.779 - ETA: 3s - loss: 0.4218 - acc: 0.779 - ETA: 3s - loss: 0.4227 - acc: 0.779 - ETA: 2s - loss: 0.4232 - acc: 0.778 - ETA: 2s - loss: 0.4191 - acc: 0.782 - ETA: 1s - loss: 0.4188 - acc: 0.781 - ETA: 1s - loss: 0.4156 - acc: 0.782 - ETA: 0s - loss: 0.4115 - acc: 0.784 - 23s 721ms/step - loss: 0.4111 - acc: 0.7853 - val_loss: 0.4188 - val_acc: 0.7890\n",
      "Epoch 8/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2228 - acc: 0.92 - ETA: 15s - loss: 0.2964 - acc: 0.88 - ETA: 15s - loss: 0.3349 - acc: 0.83 - ETA: 14s - loss: 0.3995 - acc: 0.79 - ETA: 14s - loss: 0.3849 - acc: 0.80 - ETA: 13s - loss: 0.4065 - acc: 0.79 - ETA: 13s - loss: 0.3965 - acc: 0.80 - ETA: 12s - loss: 0.4040 - acc: 0.79 - ETA: 12s - loss: 0.4042 - acc: 0.79 - ETA: 11s - loss: 0.3895 - acc: 0.80 - ETA: 11s - loss: 0.3846 - acc: 0.80 - ETA: 10s - loss: 0.3926 - acc: 0.79 - ETA: 9s - loss: 0.4138 - acc: 0.7960 - ETA: 9s - loss: 0.4127 - acc: 0.791 - ETA: 8s - loss: 0.4140 - acc: 0.787 - ETA: 8s - loss: 0.4096 - acc: 0.794 - ETA: 7s - loss: 0.4070 - acc: 0.794 - ETA: 7s - loss: 0.4080 - acc: 0.794 - ETA: 6s - loss: 0.4095 - acc: 0.790 - ETA: 6s - loss: 0.4094 - acc: 0.788 - ETA: 5s - loss: 0.4123 - acc: 0.785 - ETA: 5s - loss: 0.4139 - acc: 0.783 - ETA: 4s - loss: 0.4156 - acc: 0.783 - ETA: 4s - loss: 0.4123 - acc: 0.787 - ETA: 3s - loss: 0.4130 - acc: 0.788 - ETA: 3s - loss: 0.4139 - acc: 0.787 - ETA: 2s - loss: 0.4146 - acc: 0.785 - ETA: 2s - loss: 0.4098 - acc: 0.790 - ETA: 1s - loss: 0.4099 - acc: 0.789 - ETA: 1s - loss: 0.4075 - acc: 0.789 - ETA: 0s - loss: 0.4025 - acc: 0.793 - 23s 722ms/step - loss: 0.4015 - acc: 0.7946 - val_loss: 0.4128 - val_acc: 0.8036\n",
      "Epoch 9/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.2138 - acc: 0.91 - ETA: 15s - loss: 0.2926 - acc: 0.88 - ETA: 15s - loss: 0.3279 - acc: 0.83 - ETA: 14s - loss: 0.4334 - acc: 0.79 - ETA: 14s - loss: 0.4332 - acc: 0.77 - ETA: 13s - loss: 0.4435 - acc: 0.77 - ETA: 13s - loss: 0.4376 - acc: 0.77 - ETA: 12s - loss: 0.4424 - acc: 0.77 - ETA: 12s - loss: 0.4439 - acc: 0.76 - ETA: 11s - loss: 0.4326 - acc: 0.76 - ETA: 11s - loss: 0.4268 - acc: 0.76 - ETA: 10s - loss: 0.4321 - acc: 0.76 - ETA: 9s - loss: 0.4525 - acc: 0.7624 - ETA: 9s - loss: 0.4494 - acc: 0.759 - ETA: 8s - loss: 0.4489 - acc: 0.756 - ETA: 8s - loss: 0.4440 - acc: 0.763 - ETA: 7s - loss: 0.4412 - acc: 0.765 - ETA: 7s - loss: 0.4405 - acc: 0.767 - ETA: 6s - loss: 0.4439 - acc: 0.759 - ETA: 6s - loss: 0.4420 - acc: 0.758 - ETA: 5s - loss: 0.4438 - acc: 0.754 - ETA: 5s - loss: 0.4455 - acc: 0.755 - ETA: 4s - loss: 0.4464 - acc: 0.757 - ETA: 4s - loss: 0.4423 - acc: 0.761 - ETA: 3s - loss: 0.4425 - acc: 0.760 - ETA: 3s - loss: 0.4429 - acc: 0.760 - ETA: 2s - loss: 0.4433 - acc: 0.760 - ETA: 2s - loss: 0.4394 - acc: 0.766 - ETA: 1s - loss: 0.4381 - acc: 0.765 - ETA: 1s - loss: 0.4343 - acc: 0.767 - ETA: 0s - loss: 0.4287 - acc: 0.770 - 23s 724ms/step - loss: 0.4274 - acc: 0.7709 - val_loss: 0.4313 - val_acc: 0.7950\n",
      "Epoch 10/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2085 - acc: 0.92 - ETA: 15s - loss: 0.3926 - acc: 0.86 - ETA: 15s - loss: 0.4008 - acc: 0.80 - ETA: 15s - loss: 0.4459 - acc: 0.76 - ETA: 14s - loss: 0.4301 - acc: 0.77 - ETA: 13s - loss: 0.4431 - acc: 0.76 - ETA: 13s - loss: 0.4356 - acc: 0.77 - ETA: 12s - loss: 0.4402 - acc: 0.76 - ETA: 12s - loss: 0.4396 - acc: 0.76 - ETA: 11s - loss: 0.4265 - acc: 0.76 - ETA: 11s - loss: 0.4210 - acc: 0.77 - ETA: 10s - loss: 0.4242 - acc: 0.77 - ETA: 10s - loss: 0.4421 - acc: 0.76 - ETA: 9s - loss: 0.4384 - acc: 0.7675 - ETA: 8s - loss: 0.4382 - acc: 0.764 - ETA: 8s - loss: 0.4336 - acc: 0.770 - ETA: 7s - loss: 0.4302 - acc: 0.772 - ETA: 7s - loss: 0.4297 - acc: 0.773 - ETA: 6s - loss: 0.4348 - acc: 0.764 - ETA: 6s - loss: 0.4341 - acc: 0.763 - ETA: 5s - loss: 0.4384 - acc: 0.762 - ETA: 5s - loss: 0.4419 - acc: 0.763 - ETA: 4s - loss: 0.4441 - acc: 0.762 - ETA: 4s - loss: 0.4397 - acc: 0.763 - ETA: 3s - loss: 0.4407 - acc: 0.763 - ETA: 3s - loss: 0.4414 - acc: 0.762 - ETA: 2s - loss: 0.4418 - acc: 0.762 - ETA: 2s - loss: 0.4383 - acc: 0.767 - ETA: 1s - loss: 0.4371 - acc: 0.767 - ETA: 1s - loss: 0.4338 - acc: 0.768 - ETA: 0s - loss: 0.4290 - acc: 0.771 - 23s 725ms/step - loss: 0.4284 - acc: 0.7714 - val_loss: 0.5752 - val_acc: 0.7462\n",
      "Epoch 11/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2201 - acc: 0.93 - ETA: 15s - loss: 0.3114 - acc: 0.87 - ETA: 15s - loss: 0.3380 - acc: 0.81 - ETA: 14s - loss: 0.4011 - acc: 0.78 - ETA: 14s - loss: 0.3860 - acc: 0.78 - ETA: 13s - loss: 0.4061 - acc: 0.77 - ETA: 13s - loss: 0.3955 - acc: 0.78 - ETA: 12s - loss: 0.4032 - acc: 0.78 - ETA: 12s - loss: 0.4043 - acc: 0.77 - ETA: 11s - loss: 0.3909 - acc: 0.78 - ETA: 11s - loss: 0.3877 - acc: 0.78 - ETA: 10s - loss: 0.3939 - acc: 0.77 - ETA: 10s - loss: 0.4154 - acc: 0.77 - ETA: 9s - loss: 0.4131 - acc: 0.7759 - ETA: 8s - loss: 0.4144 - acc: 0.772 - ETA: 8s - loss: 0.4095 - acc: 0.779 - ETA: 7s - loss: 0.4064 - acc: 0.781 - ETA: 7s - loss: 0.4071 - acc: 0.782 - ETA: 6s - loss: 0.4099 - acc: 0.777 - ETA: 6s - loss: 0.4094 - acc: 0.776 - ETA: 5s - loss: 0.4127 - acc: 0.772 - ETA: 5s - loss: 0.4151 - acc: 0.773 - ETA: 4s - loss: 0.4174 - acc: 0.771 - ETA: 4s - loss: 0.4144 - acc: 0.775 - ETA: 3s - loss: 0.4151 - acc: 0.776 - ETA: 3s - loss: 0.4160 - acc: 0.776 - ETA: 2s - loss: 0.4166 - acc: 0.775 - ETA: 2s - loss: 0.4129 - acc: 0.779 - ETA: 1s - loss: 0.4121 - acc: 0.780 - ETA: 1s - loss: 0.4088 - acc: 0.781 - ETA: 0s - loss: 0.4036 - acc: 0.785 - 23s 724ms/step - loss: 0.4030 - acc: 0.7856 - val_loss: 0.4195 - val_acc: 0.7933\n",
      "Epoch 12/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2006 - acc: 0.93 - ETA: 15s - loss: 0.2783 - acc: 0.89 - ETA: 15s - loss: 0.3218 - acc: 0.83 - ETA: 14s - loss: 0.3795 - acc: 0.79 - ETA: 14s - loss: 0.3647 - acc: 0.80 - ETA: 13s - loss: 0.3946 - acc: 0.79 - ETA: 13s - loss: 0.3838 - acc: 0.80 - ETA: 12s - loss: 0.3911 - acc: 0.79 - ETA: 12s - loss: 0.3926 - acc: 0.79 - ETA: 11s - loss: 0.3793 - acc: 0.79 - ETA: 11s - loss: 0.3762 - acc: 0.79 - ETA: 10s - loss: 0.3823 - acc: 0.79 - ETA: 9s - loss: 0.4043 - acc: 0.7926 - ETA: 9s - loss: 0.4022 - acc: 0.790 - ETA: 8s - loss: 0.4041 - acc: 0.786 - ETA: 8s - loss: 0.3988 - acc: 0.793 - ETA: 7s - loss: 0.3957 - acc: 0.794 - ETA: 7s - loss: 0.3962 - acc: 0.794 - ETA: 6s - loss: 0.3971 - acc: 0.793 - ETA: 6s - loss: 0.3972 - acc: 0.791 - ETA: 5s - loss: 0.4010 - acc: 0.787 - ETA: 5s - loss: 0.4029 - acc: 0.787 - ETA: 4s - loss: 0.4052 - acc: 0.786 - ETA: 4s - loss: 0.4024 - acc: 0.789 - ETA: 3s - loss: 0.4027 - acc: 0.791 - ETA: 3s - loss: 0.4040 - acc: 0.790 - ETA: 2s - loss: 0.4046 - acc: 0.789 - ETA: 2s - loss: 0.4009 - acc: 0.793 - ETA: 1s - loss: 0.4003 - acc: 0.793 - ETA: 1s - loss: 0.3970 - acc: 0.794 - ETA: 0s - loss: 0.3919 - acc: 0.798 - 23s 721ms/step - loss: 0.3914 - acc: 0.7984 - val_loss: 0.4068 - val_acc: 0.8004\n",
      "Epoch 13/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 16s - loss: 0.1944 - acc: 0.93 - ETA: 15s - loss: 0.2685 - acc: 0.89 - ETA: 15s - loss: 0.3178 - acc: 0.83 - ETA: 14s - loss: 0.3712 - acc: 0.79 - ETA: 14s - loss: 0.3600 - acc: 0.80 - ETA: 13s - loss: 0.3842 - acc: 0.79 - ETA: 13s - loss: 0.3740 - acc: 0.80 - ETA: 12s - loss: 0.3811 - acc: 0.79 - ETA: 12s - loss: 0.3825 - acc: 0.79 - ETA: 11s - loss: 0.3696 - acc: 0.79 - ETA: 10s - loss: 0.3668 - acc: 0.80 - ETA: 10s - loss: 0.3737 - acc: 0.79 - ETA: 9s - loss: 0.3955 - acc: 0.7947 - ETA: 9s - loss: 0.3937 - acc: 0.792 - ETA: 8s - loss: 0.3959 - acc: 0.788 - ETA: 8s - loss: 0.3907 - acc: 0.795 - ETA: 7s - loss: 0.3877 - acc: 0.797 - ETA: 7s - loss: 0.3881 - acc: 0.797 - ETA: 6s - loss: 0.3891 - acc: 0.796 - ETA: 6s - loss: 0.3894 - acc: 0.794 - ETA: 5s - loss: 0.3928 - acc: 0.791 - ETA: 5s - loss: 0.3941 - acc: 0.791 - ETA: 4s - loss: 0.3964 - acc: 0.790 - ETA: 4s - loss: 0.3937 - acc: 0.793 - ETA: 3s - loss: 0.3937 - acc: 0.796 - ETA: 3s - loss: 0.3952 - acc: 0.795 - ETA: 2s - loss: 0.3959 - acc: 0.794 - ETA: 2s - loss: 0.3922 - acc: 0.798 - ETA: 1s - loss: 0.3918 - acc: 0.798 - ETA: 1s - loss: 0.3886 - acc: 0.799 - ETA: 0s - loss: 0.3838 - acc: 0.803 - 23s 723ms/step - loss: 0.3834 - acc: 0.8033 - val_loss: 0.3938 - val_acc: 0.8065\n",
      "Epoch 14/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.1939 - acc: 0.93 - ETA: 15s - loss: 0.2630 - acc: 0.90 - ETA: 15s - loss: 0.3166 - acc: 0.84 - ETA: 14s - loss: 0.3688 - acc: 0.80 - ETA: 14s - loss: 0.3549 - acc: 0.80 - ETA: 13s - loss: 0.3788 - acc: 0.79 - ETA: 13s - loss: 0.3685 - acc: 0.80 - ETA: 12s - loss: 0.3756 - acc: 0.80 - ETA: 12s - loss: 0.3772 - acc: 0.79 - ETA: 11s - loss: 0.3642 - acc: 0.80 - ETA: 11s - loss: 0.3613 - acc: 0.80 - ETA: 10s - loss: 0.3683 - acc: 0.80 - ETA: 9s - loss: 0.3890 - acc: 0.8010 - ETA: 9s - loss: 0.3874 - acc: 0.798 - ETA: 8s - loss: 0.3898 - acc: 0.795 - ETA: 8s - loss: 0.3845 - acc: 0.801 - ETA: 7s - loss: 0.3817 - acc: 0.802 - ETA: 7s - loss: 0.3820 - acc: 0.803 - ETA: 6s - loss: 0.3835 - acc: 0.801 - ETA: 6s - loss: 0.3839 - acc: 0.799 - ETA: 5s - loss: 0.3868 - acc: 0.796 - ETA: 5s - loss: 0.3877 - acc: 0.796 - ETA: 4s - loss: 0.3895 - acc: 0.796 - ETA: 4s - loss: 0.3867 - acc: 0.799 - ETA: 3s - loss: 0.3866 - acc: 0.802 - ETA: 3s - loss: 0.3882 - acc: 0.801 - ETA: 2s - loss: 0.3888 - acc: 0.800 - ETA: 2s - loss: 0.3850 - acc: 0.804 - ETA: 1s - loss: 0.3849 - acc: 0.804 - ETA: 1s - loss: 0.3819 - acc: 0.805 - ETA: 0s - loss: 0.3772 - acc: 0.808 - 23s 723ms/step - loss: 0.3767 - acc: 0.8093 - val_loss: 0.3864 - val_acc: 0.8109\n",
      "Epoch 15/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.1956 - acc: 0.93 - ETA: 15s - loss: 0.2660 - acc: 0.89 - ETA: 15s - loss: 0.3190 - acc: 0.83 - ETA: 14s - loss: 0.3717 - acc: 0.79 - ETA: 14s - loss: 0.3576 - acc: 0.79 - ETA: 13s - loss: 0.3815 - acc: 0.78 - ETA: 13s - loss: 0.3724 - acc: 0.79 - ETA: 12s - loss: 0.3785 - acc: 0.79 - ETA: 12s - loss: 0.3800 - acc: 0.79 - ETA: 11s - loss: 0.3672 - acc: 0.79 - ETA: 10s - loss: 0.3639 - acc: 0.79 - ETA: 10s - loss: 0.3710 - acc: 0.79 - ETA: 9s - loss: 0.3902 - acc: 0.7960 - ETA: 9s - loss: 0.3881 - acc: 0.794 - ETA: 8s - loss: 0.3905 - acc: 0.790 - ETA: 8s - loss: 0.3851 - acc: 0.797 - ETA: 7s - loss: 0.3820 - acc: 0.799 - ETA: 7s - loss: 0.3820 - acc: 0.800 - ETA: 6s - loss: 0.3835 - acc: 0.798 - ETA: 6s - loss: 0.3840 - acc: 0.796 - ETA: 5s - loss: 0.3863 - acc: 0.794 - ETA: 5s - loss: 0.3871 - acc: 0.794 - ETA: 4s - loss: 0.3883 - acc: 0.794 - ETA: 4s - loss: 0.3852 - acc: 0.797 - ETA: 3s - loss: 0.3848 - acc: 0.800 - ETA: 3s - loss: 0.3864 - acc: 0.800 - ETA: 2s - loss: 0.3869 - acc: 0.799 - ETA: 2s - loss: 0.3830 - acc: 0.803 - ETA: 1s - loss: 0.3832 - acc: 0.803 - ETA: 1s - loss: 0.3800 - acc: 0.804 - ETA: 0s - loss: 0.3751 - acc: 0.808 - 23s 726ms/step - loss: 0.3746 - acc: 0.8086 - val_loss: 0.3832 - val_acc: 0.8150\n",
      "Epoch 16/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1934 - acc: 0.94 - ETA: 15s - loss: 0.2576 - acc: 0.90 - ETA: 15s - loss: 0.3160 - acc: 0.84 - ETA: 14s - loss: 0.3676 - acc: 0.81 - ETA: 14s - loss: 0.3520 - acc: 0.82 - ETA: 13s - loss: 0.3722 - acc: 0.81 - ETA: 13s - loss: 0.3605 - acc: 0.81 - ETA: 12s - loss: 0.3674 - acc: 0.81 - ETA: 12s - loss: 0.3688 - acc: 0.81 - ETA: 11s - loss: 0.3560 - acc: 0.81 - ETA: 11s - loss: 0.3529 - acc: 0.81 - ETA: 10s - loss: 0.3613 - acc: 0.81 - ETA: 9s - loss: 0.3808 - acc: 0.8118 - ETA: 9s - loss: 0.3794 - acc: 0.809 - ETA: 8s - loss: 0.3823 - acc: 0.805 - ETA: 8s - loss: 0.3770 - acc: 0.811 - ETA: 7s - loss: 0.3742 - acc: 0.812 - ETA: 7s - loss: 0.3745 - acc: 0.813 - ETA: 6s - loss: 0.3762 - acc: 0.810 - ETA: 6s - loss: 0.3772 - acc: 0.808 - ETA: 5s - loss: 0.3793 - acc: 0.806 - ETA: 5s - loss: 0.3798 - acc: 0.805 - ETA: 4s - loss: 0.3807 - acc: 0.805 - ETA: 4s - loss: 0.3775 - acc: 0.808 - ETA: 3s - loss: 0.3768 - acc: 0.811 - ETA: 3s - loss: 0.3785 - acc: 0.810 - ETA: 2s - loss: 0.3791 - acc: 0.809 - ETA: 2s - loss: 0.3755 - acc: 0.812 - ETA: 1s - loss: 0.3759 - acc: 0.812 - ETA: 1s - loss: 0.3732 - acc: 0.813 - ETA: 0s - loss: 0.3681 - acc: 0.816 - 23s 727ms/step - loss: 0.3675 - acc: 0.8172 - val_loss: 0.3802 - val_acc: 0.8163\n",
      "Epoch 17/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1957 - acc: 0.93 - ETA: 15s - loss: 0.2495 - acc: 0.91 - ETA: 15s - loss: 0.3115 - acc: 0.84 - ETA: 14s - loss: 0.3619 - acc: 0.82 - ETA: 14s - loss: 0.3450 - acc: 0.82 - ETA: 13s - loss: 0.3667 - acc: 0.81 - ETA: 13s - loss: 0.3570 - acc: 0.82 - ETA: 12s - loss: 0.3637 - acc: 0.81 - ETA: 12s - loss: 0.3661 - acc: 0.81 - ETA: 11s - loss: 0.3538 - acc: 0.81 - ETA: 11s - loss: 0.3506 - acc: 0.82 - ETA: 10s - loss: 0.3585 - acc: 0.81 - ETA: 10s - loss: 0.3772 - acc: 0.81 - ETA: 9s - loss: 0.3758 - acc: 0.8108 - ETA: 8s - loss: 0.3787 - acc: 0.807 - ETA: 8s - loss: 0.3731 - acc: 0.813 - ETA: 7s - loss: 0.3704 - acc: 0.814 - ETA: 7s - loss: 0.3709 - acc: 0.815 - ETA: 6s - loss: 0.3727 - acc: 0.812 - ETA: 6s - loss: 0.3737 - acc: 0.810 - ETA: 5s - loss: 0.3758 - acc: 0.807 - ETA: 5s - loss: 0.3756 - acc: 0.807 - ETA: 4s - loss: 0.3756 - acc: 0.808 - ETA: 4s - loss: 0.3725 - acc: 0.810 - ETA: 3s - loss: 0.3719 - acc: 0.813 - ETA: 3s - loss: 0.3729 - acc: 0.812 - ETA: 2s - loss: 0.3733 - acc: 0.811 - ETA: 2s - loss: 0.3688 - acc: 0.815 - ETA: 1s - loss: 0.3709 - acc: 0.815 - ETA: 1s - loss: 0.3687 - acc: 0.815 - ETA: 0s - loss: 0.3639 - acc: 0.819 - 23s 726ms/step - loss: 0.3632 - acc: 0.8201 - val_loss: 0.3852 - val_acc: 0.8163\n",
      "Epoch 18/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1969 - acc: 0.93 - ETA: 15s - loss: 0.2485 - acc: 0.91 - ETA: 15s - loss: 0.3158 - acc: 0.84 - ETA: 14s - loss: 0.3622 - acc: 0.81 - ETA: 14s - loss: 0.3479 - acc: 0.82 - ETA: 13s - loss: 0.3657 - acc: 0.81 - ETA: 13s - loss: 0.3541 - acc: 0.82 - ETA: 12s - loss: 0.3612 - acc: 0.81 - ETA: 12s - loss: 0.3636 - acc: 0.81 - ETA: 11s - loss: 0.3514 - acc: 0.81 - ETA: 11s - loss: 0.3482 - acc: 0.82 - ETA: 10s - loss: 0.3561 - acc: 0.81 - ETA: 10s - loss: 0.3753 - acc: 0.81 - ETA: 9s - loss: 0.3743 - acc: 0.8111 - ETA: 8s - loss: 0.3769 - acc: 0.808 - ETA: 8s - loss: 0.3714 - acc: 0.814 - ETA: 7s - loss: 0.3682 - acc: 0.816 - ETA: 7s - loss: 0.3687 - acc: 0.816 - ETA: 6s - loss: 0.3708 - acc: 0.814 - ETA: 6s - loss: 0.3718 - acc: 0.811 - ETA: 5s - loss: 0.3740 - acc: 0.808 - ETA: 5s - loss: 0.3739 - acc: 0.808 - ETA: 4s - loss: 0.3737 - acc: 0.809 - ETA: 4s - loss: 0.3704 - acc: 0.812 - ETA: 3s - loss: 0.3697 - acc: 0.815 - ETA: 3s - loss: 0.3702 - acc: 0.814 - ETA: 2s - loss: 0.3720 - acc: 0.812 - ETA: 2s - loss: 0.3673 - acc: 0.816 - ETA: 1s - loss: 0.3677 - acc: 0.816 - ETA: 1s - loss: 0.3652 - acc: 0.817 - ETA: 0s - loss: 0.3617 - acc: 0.820 - 23s 726ms/step - loss: 0.3615 - acc: 0.8206 - val_loss: 1.2423 - val_acc: 0.7548\n",
      "Epoch 19/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 16s - loss: 0.2194 - acc: 0.89 - ETA: 16s - loss: 0.2624 - acc: 0.89 - ETA: 15s - loss: 0.3186 - acc: 0.83 - ETA: 14s - loss: 0.3767 - acc: 0.80 - ETA: 14s - loss: 0.3575 - acc: 0.81 - ETA: 13s - loss: 0.3810 - acc: 0.80 - ETA: 13s - loss: 0.3692 - acc: 0.81 - ETA: 12s - loss: 0.3753 - acc: 0.81 - ETA: 12s - loss: 0.3770 - acc: 0.80 - ETA: 11s - loss: 0.3630 - acc: 0.81 - ETA: 11s - loss: 0.3590 - acc: 0.81 - ETA: 10s - loss: 0.3649 - acc: 0.81 - ETA: 10s - loss: 0.3826 - acc: 0.81 - ETA: 9s - loss: 0.3809 - acc: 0.8075 - ETA: 9s - loss: 0.3828 - acc: 0.804 - ETA: 8s - loss: 0.3768 - acc: 0.811 - ETA: 7s - loss: 0.3732 - acc: 0.813 - ETA: 7s - loss: 0.3728 - acc: 0.814 - ETA: 6s - loss: 0.3739 - acc: 0.812 - ETA: 6s - loss: 0.3749 - acc: 0.810 - ETA: 5s - loss: 0.3766 - acc: 0.808 - ETA: 5s - loss: 0.3766 - acc: 0.807 - ETA: 4s - loss: 0.3766 - acc: 0.808 - ETA: 4s - loss: 0.3730 - acc: 0.810 - ETA: 3s - loss: 0.3721 - acc: 0.812 - ETA: 3s - loss: 0.3733 - acc: 0.811 - ETA: 2s - loss: 0.3737 - acc: 0.810 - ETA: 2s - loss: 0.3690 - acc: 0.814 - ETA: 1s - loss: 0.3699 - acc: 0.814 - ETA: 1s - loss: 0.3669 - acc: 0.815 - ETA: 0s - loss: 0.3634 - acc: 0.818 - 23s 727ms/step - loss: 0.3625 - acc: 0.8190 - val_loss: 0.4518 - val_acc: 0.8012\n",
      "Epoch 20/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2030 - acc: 0.92 - ETA: 15s - loss: 0.2493 - acc: 0.90 - ETA: 15s - loss: 0.3103 - acc: 0.84 - ETA: 14s - loss: 0.3619 - acc: 0.81 - ETA: 14s - loss: 0.3450 - acc: 0.82 - ETA: 13s - loss: 0.3643 - acc: 0.81 - ETA: 13s - loss: 0.3532 - acc: 0.82 - ETA: 12s - loss: 0.3594 - acc: 0.81 - ETA: 12s - loss: 0.3614 - acc: 0.81 - ETA: 11s - loss: 0.3480 - acc: 0.82 - ETA: 11s - loss: 0.3448 - acc: 0.82 - ETA: 10s - loss: 0.3520 - acc: 0.81 - ETA: 9s - loss: 0.3696 - acc: 0.8162 - ETA: 9s - loss: 0.3685 - acc: 0.813 - ETA: 8s - loss: 0.3710 - acc: 0.810 - ETA: 8s - loss: 0.3655 - acc: 0.817 - ETA: 7s - loss: 0.3624 - acc: 0.818 - ETA: 7s - loss: 0.3625 - acc: 0.819 - ETA: 6s - loss: 0.3642 - acc: 0.817 - ETA: 6s - loss: 0.3652 - acc: 0.815 - ETA: 5s - loss: 0.3670 - acc: 0.812 - ETA: 5s - loss: 0.3668 - acc: 0.812 - ETA: 4s - loss: 0.3661 - acc: 0.813 - ETA: 4s - loss: 0.3633 - acc: 0.816 - ETA: 3s - loss: 0.3617 - acc: 0.818 - ETA: 3s - loss: 0.3630 - acc: 0.818 - ETA: 2s - loss: 0.3640 - acc: 0.816 - ETA: 2s - loss: 0.3604 - acc: 0.819 - ETA: 1s - loss: 0.3604 - acc: 0.819 - ETA: 1s - loss: 0.3577 - acc: 0.820 - ETA: 0s - loss: 0.3529 - acc: 0.824 - 23s 725ms/step - loss: 0.3523 - acc: 0.8244 - val_loss: 0.3764 - val_acc: 0.8200\n",
      "Epoch 21/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.2043 - acc: 0.92 - ETA: 15s - loss: 0.2318 - acc: 0.92 - ETA: 15s - loss: 0.3006 - acc: 0.86 - ETA: 14s - loss: 0.3513 - acc: 0.82 - ETA: 14s - loss: 0.3360 - acc: 0.84 - ETA: 13s - loss: 0.3535 - acc: 0.83 - ETA: 13s - loss: 0.3437 - acc: 0.83 - ETA: 12s - loss: 0.3504 - acc: 0.83 - ETA: 12s - loss: 0.3529 - acc: 0.82 - ETA: 11s - loss: 0.3404 - acc: 0.83 - ETA: 11s - loss: 0.3373 - acc: 0.83 - ETA: 10s - loss: 0.3445 - acc: 0.82 - ETA: 9s - loss: 0.3626 - acc: 0.8256 - ETA: 9s - loss: 0.3617 - acc: 0.822 - ETA: 8s - loss: 0.3645 - acc: 0.819 - ETA: 8s - loss: 0.3588 - acc: 0.825 - ETA: 7s - loss: 0.3561 - acc: 0.826 - ETA: 7s - loss: 0.3564 - acc: 0.826 - ETA: 6s - loss: 0.3576 - acc: 0.824 - ETA: 6s - loss: 0.3585 - acc: 0.822 - ETA: 5s - loss: 0.3600 - acc: 0.820 - ETA: 5s - loss: 0.3606 - acc: 0.820 - ETA: 4s - loss: 0.3595 - acc: 0.821 - ETA: 4s - loss: 0.3571 - acc: 0.823 - ETA: 3s - loss: 0.3554 - acc: 0.825 - ETA: 3s - loss: 0.3571 - acc: 0.824 - ETA: 2s - loss: 0.3586 - acc: 0.822 - ETA: 2s - loss: 0.3548 - acc: 0.825 - ETA: 1s - loss: 0.3546 - acc: 0.825 - ETA: 1s - loss: 0.3521 - acc: 0.826 - ETA: 0s - loss: 0.3475 - acc: 0.829 - 23s 725ms/step - loss: 0.3468 - acc: 0.8305 - val_loss: 0.4518 - val_acc: 0.7816\n",
      "Epoch 22/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2085 - acc: 0.92 - ETA: 15s - loss: 0.2326 - acc: 0.92 - ETA: 15s - loss: 0.3028 - acc: 0.85 - ETA: 14s - loss: 0.3476 - acc: 0.82 - ETA: 14s - loss: 0.3319 - acc: 0.84 - ETA: 13s - loss: 0.3494 - acc: 0.83 - ETA: 13s - loss: 0.3389 - acc: 0.83 - ETA: 12s - loss: 0.3459 - acc: 0.83 - ETA: 12s - loss: 0.3483 - acc: 0.82 - ETA: 11s - loss: 0.3359 - acc: 0.83 - ETA: 11s - loss: 0.3328 - acc: 0.83 - ETA: 10s - loss: 0.3400 - acc: 0.83 - ETA: 9s - loss: 0.3578 - acc: 0.8275 - ETA: 9s - loss: 0.3568 - acc: 0.824 - ETA: 8s - loss: 0.3594 - acc: 0.821 - ETA: 8s - loss: 0.3538 - acc: 0.827 - ETA: 7s - loss: 0.3512 - acc: 0.828 - ETA: 7s - loss: 0.3513 - acc: 0.828 - ETA: 6s - loss: 0.3525 - acc: 0.827 - ETA: 6s - loss: 0.3528 - acc: 0.825 - ETA: 5s - loss: 0.3546 - acc: 0.823 - ETA: 5s - loss: 0.3552 - acc: 0.823 - ETA: 4s - loss: 0.3561 - acc: 0.822 - ETA: 4s - loss: 0.3554 - acc: 0.823 - ETA: 3s - loss: 0.3556 - acc: 0.824 - ETA: 3s - loss: 0.3565 - acc: 0.823 - ETA: 2s - loss: 0.3580 - acc: 0.821 - ETA: 2s - loss: 0.3535 - acc: 0.824 - ETA: 1s - loss: 0.3540 - acc: 0.824 - ETA: 1s - loss: 0.3511 - acc: 0.825 - ETA: 0s - loss: 0.3469 - acc: 0.827 - 23s 725ms/step - loss: 0.3465 - acc: 0.8277 - val_loss: 0.5803 - val_acc: 0.8069\n",
      "Epoch 23/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1952 - acc: 0.90 - ETA: 16s - loss: 0.2428 - acc: 0.90 - ETA: 15s - loss: 0.3175 - acc: 0.83 - ETA: 14s - loss: 0.3733 - acc: 0.80 - ETA: 14s - loss: 0.3540 - acc: 0.82 - ETA: 13s - loss: 0.3672 - acc: 0.82 - ETA: 13s - loss: 0.3566 - acc: 0.82 - ETA: 12s - loss: 0.3625 - acc: 0.82 - ETA: 12s - loss: 0.3637 - acc: 0.81 - ETA: 11s - loss: 0.3498 - acc: 0.82 - ETA: 11s - loss: 0.3460 - acc: 0.82 - ETA: 10s - loss: 0.3529 - acc: 0.82 - ETA: 10s - loss: 0.3704 - acc: 0.82 - ETA: 9s - loss: 0.3693 - acc: 0.8179 - ETA: 8s - loss: 0.3710 - acc: 0.815 - ETA: 8s - loss: 0.3650 - acc: 0.822 - ETA: 7s - loss: 0.3617 - acc: 0.823 - ETA: 7s - loss: 0.3614 - acc: 0.823 - ETA: 6s - loss: 0.3620 - acc: 0.821 - ETA: 6s - loss: 0.3624 - acc: 0.819 - ETA: 5s - loss: 0.3632 - acc: 0.818 - ETA: 5s - loss: 0.3628 - acc: 0.818 - ETA: 4s - loss: 0.3625 - acc: 0.819 - ETA: 4s - loss: 0.3599 - acc: 0.821 - ETA: 3s - loss: 0.3582 - acc: 0.823 - ETA: 3s - loss: 0.3587 - acc: 0.822 - ETA: 2s - loss: 0.3591 - acc: 0.822 - ETA: 2s - loss: 0.3532 - acc: 0.826 - ETA: 1s - loss: 0.3529 - acc: 0.826 - ETA: 1s - loss: 0.3521 - acc: 0.826 - ETA: 0s - loss: 0.3486 - acc: 0.828 - 23s 725ms/step - loss: 0.3480 - acc: 0.8298 - val_loss: 0.6168 - val_acc: 0.8083\n",
      "Epoch 24/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.2286 - acc: 0.89 - ETA: 15s - loss: 0.2464 - acc: 0.89 - ETA: 15s - loss: 0.3101 - acc: 0.84 - ETA: 14s - loss: 0.3629 - acc: 0.81 - ETA: 14s - loss: 0.3417 - acc: 0.83 - ETA: 13s - loss: 0.3583 - acc: 0.82 - ETA: 13s - loss: 0.3471 - acc: 0.83 - ETA: 12s - loss: 0.3530 - acc: 0.82 - ETA: 12s - loss: 0.3550 - acc: 0.82 - ETA: 11s - loss: 0.3413 - acc: 0.82 - ETA: 11s - loss: 0.3384 - acc: 0.83 - ETA: 10s - loss: 0.3438 - acc: 0.82 - ETA: 10s - loss: 0.3603 - acc: 0.82 - ETA: 9s - loss: 0.3594 - acc: 0.8214 - ETA: 8s - loss: 0.3613 - acc: 0.819 - ETA: 8s - loss: 0.3550 - acc: 0.825 - ETA: 7s - loss: 0.3521 - acc: 0.826 - ETA: 7s - loss: 0.3517 - acc: 0.827 - ETA: 6s - loss: 0.3526 - acc: 0.826 - ETA: 6s - loss: 0.3533 - acc: 0.824 - ETA: 5s - loss: 0.3540 - acc: 0.822 - ETA: 5s - loss: 0.3541 - acc: 0.821 - ETA: 4s - loss: 0.3540 - acc: 0.822 - ETA: 4s - loss: 0.3511 - acc: 0.824 - ETA: 3s - loss: 0.3499 - acc: 0.825 - ETA: 3s - loss: 0.3513 - acc: 0.825 - ETA: 2s - loss: 0.3510 - acc: 0.825 - ETA: 2s - loss: 0.3465 - acc: 0.828 - ETA: 1s - loss: 0.3474 - acc: 0.828 - ETA: 1s - loss: 0.3440 - acc: 0.829 - ETA: 0s - loss: 0.3386 - acc: 0.833 - 23s 725ms/step - loss: 0.3379 - acc: 0.8337 - val_loss: 0.4500 - val_acc: 0.7670\n",
      "Epoch 25/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 16s - loss: 0.2139 - acc: 0.91 - ETA: 15s - loss: 0.2383 - acc: 0.90 - ETA: 15s - loss: 0.3120 - acc: 0.84 - ETA: 14s - loss: 0.3557 - acc: 0.81 - ETA: 14s - loss: 0.3345 - acc: 0.83 - ETA: 13s - loss: 0.3478 - acc: 0.82 - ETA: 13s - loss: 0.3363 - acc: 0.83 - ETA: 12s - loss: 0.3426 - acc: 0.82 - ETA: 12s - loss: 0.3443 - acc: 0.82 - ETA: 11s - loss: 0.3306 - acc: 0.83 - ETA: 11s - loss: 0.3290 - acc: 0.83 - ETA: 10s - loss: 0.3358 - acc: 0.82 - ETA: 10s - loss: 0.3518 - acc: 0.82 - ETA: 9s - loss: 0.3525 - acc: 0.8236 - ETA: 8s - loss: 0.3544 - acc: 0.821 - ETA: 8s - loss: 0.3489 - acc: 0.827 - ETA: 7s - loss: 0.3461 - acc: 0.829 - ETA: 7s - loss: 0.3452 - acc: 0.830 - ETA: 6s - loss: 0.3458 - acc: 0.829 - ETA: 6s - loss: 0.3455 - acc: 0.827 - ETA: 5s - loss: 0.3469 - acc: 0.826 - ETA: 5s - loss: 0.3487 - acc: 0.825 - ETA: 4s - loss: 0.3492 - acc: 0.826 - ETA: 4s - loss: 0.3454 - acc: 0.828 - ETA: 3s - loss: 0.3442 - acc: 0.829 - ETA: 3s - loss: 0.3434 - acc: 0.830 - ETA: 2s - loss: 0.3436 - acc: 0.829 - ETA: 2s - loss: 0.3377 - acc: 0.833 - ETA: 1s - loss: 0.3369 - acc: 0.833 - ETA: 1s - loss: 0.3333 - acc: 0.834 - ETA: 0s - loss: 0.3282 - acc: 0.838 - 23s 725ms/step - loss: 0.3274 - acc: 0.8385 - val_loss: 0.3720 - val_acc: 0.8211\n",
      "Epoch 26/32\n",
      "32/32 [==============================] - ETA: 15s - loss: 0.1995 - acc: 0.92 - ETA: 15s - loss: 0.2288 - acc: 0.91 - ETA: 15s - loss: 0.3094 - acc: 0.84 - ETA: 14s - loss: 0.3473 - acc: 0.82 - ETA: 14s - loss: 0.3322 - acc: 0.83 - ETA: 13s - loss: 0.3454 - acc: 0.82 - ETA: 13s - loss: 0.3330 - acc: 0.83 - ETA: 12s - loss: 0.3386 - acc: 0.83 - ETA: 12s - loss: 0.3396 - acc: 0.82 - ETA: 11s - loss: 0.3252 - acc: 0.83 - ETA: 11s - loss: 0.3230 - acc: 0.83 - ETA: 10s - loss: 0.3313 - acc: 0.83 - ETA: 10s - loss: 0.3459 - acc: 0.83 - ETA: 9s - loss: 0.3471 - acc: 0.8268 - ETA: 8s - loss: 0.3492 - acc: 0.825 - ETA: 8s - loss: 0.3439 - acc: 0.831 - ETA: 7s - loss: 0.3408 - acc: 0.832 - ETA: 7s - loss: 0.3404 - acc: 0.833 - ETA: 6s - loss: 0.3420 - acc: 0.831 - ETA: 6s - loss: 0.3432 - acc: 0.829 - ETA: 5s - loss: 0.3435 - acc: 0.827 - ETA: 5s - loss: 0.3432 - acc: 0.827 - ETA: 4s - loss: 0.3420 - acc: 0.828 - ETA: 4s - loss: 0.3373 - acc: 0.832 - ETA: 3s - loss: 0.3351 - acc: 0.834 - ETA: 3s - loss: 0.3355 - acc: 0.834 - ETA: 2s - loss: 0.3342 - acc: 0.835 - ETA: 2s - loss: 0.3312 - acc: 0.837 - ETA: 1s - loss: 0.3319 - acc: 0.835 - ETA: 1s - loss: 0.3291 - acc: 0.837 - ETA: 0s - loss: 0.3242 - acc: 0.840 - 23s 725ms/step - loss: 0.3237 - acc: 0.8411 - val_loss: 0.3770 - val_acc: 0.8194\n",
      "Epoch 27/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1887 - acc: 0.92 - ETA: 15s - loss: 0.2228 - acc: 0.91 - ETA: 15s - loss: 0.2975 - acc: 0.85 - ETA: 14s - loss: 0.3392 - acc: 0.82 - ETA: 14s - loss: 0.3169 - acc: 0.84 - ETA: 13s - loss: 0.3271 - acc: 0.84 - ETA: 13s - loss: 0.3163 - acc: 0.84 - ETA: 12s - loss: 0.3236 - acc: 0.84 - ETA: 12s - loss: 0.3257 - acc: 0.84 - ETA: 11s - loss: 0.3118 - acc: 0.84 - ETA: 11s - loss: 0.3082 - acc: 0.85 - ETA: 10s - loss: 0.3155 - acc: 0.84 - ETA: 10s - loss: 0.3296 - acc: 0.84 - ETA: 9s - loss: 0.3317 - acc: 0.8372 - ETA: 8s - loss: 0.3345 - acc: 0.835 - ETA: 8s - loss: 0.3293 - acc: 0.840 - ETA: 7s - loss: 0.3267 - acc: 0.842 - ETA: 7s - loss: 0.3268 - acc: 0.842 - ETA: 6s - loss: 0.3290 - acc: 0.840 - ETA: 6s - loss: 0.3310 - acc: 0.838 - ETA: 5s - loss: 0.3340 - acc: 0.833 - ETA: 5s - loss: 0.3345 - acc: 0.834 - ETA: 4s - loss: 0.3331 - acc: 0.835 - ETA: 4s - loss: 0.3304 - acc: 0.837 - ETA: 3s - loss: 0.3286 - acc: 0.839 - ETA: 3s - loss: 0.3303 - acc: 0.838 - ETA: 2s - loss: 0.3318 - acc: 0.838 - ETA: 2s - loss: 0.3276 - acc: 0.841 - ETA: 1s - loss: 0.3275 - acc: 0.840 - ETA: 1s - loss: 0.3245 - acc: 0.841 - ETA: 0s - loss: 0.3194 - acc: 0.845 - 23s 727ms/step - loss: 0.3185 - acc: 0.8457 - val_loss: 0.3485 - val_acc: 0.8345\n",
      "Epoch 28/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1674 - acc: 0.93 - ETA: 15s - loss: 0.2154 - acc: 0.92 - ETA: 15s - loss: 0.3048 - acc: 0.85 - ETA: 14s - loss: 0.3440 - acc: 0.82 - ETA: 14s - loss: 0.3277 - acc: 0.83 - ETA: 13s - loss: 0.3384 - acc: 0.83 - ETA: 13s - loss: 0.3257 - acc: 0.83 - ETA: 12s - loss: 0.3324 - acc: 0.83 - ETA: 12s - loss: 0.3349 - acc: 0.83 - ETA: 11s - loss: 0.3202 - acc: 0.83 - ETA: 11s - loss: 0.3235 - acc: 0.83 - ETA: 10s - loss: 0.3312 - acc: 0.83 - ETA: 10s - loss: 0.3499 - acc: 0.82 - ETA: 9s - loss: 0.3528 - acc: 0.8218 - ETA: 8s - loss: 0.3552 - acc: 0.818 - ETA: 8s - loss: 0.3492 - acc: 0.825 - ETA: 7s - loss: 0.3463 - acc: 0.826 - ETA: 7s - loss: 0.3463 - acc: 0.827 - ETA: 6s - loss: 0.3465 - acc: 0.826 - ETA: 6s - loss: 0.3467 - acc: 0.825 - ETA: 5s - loss: 0.3469 - acc: 0.823 - ETA: 5s - loss: 0.3471 - acc: 0.823 - ETA: 4s - loss: 0.3463 - acc: 0.824 - ETA: 4s - loss: 0.3426 - acc: 0.826 - ETA: 3s - loss: 0.3404 - acc: 0.828 - ETA: 3s - loss: 0.3390 - acc: 0.829 - ETA: 2s - loss: 0.3381 - acc: 0.829 - ETA: 2s - loss: 0.3324 - acc: 0.833 - ETA: 1s - loss: 0.3313 - acc: 0.833 - ETA: 1s - loss: 0.3273 - acc: 0.835 - ETA: 0s - loss: 0.3222 - acc: 0.839 - 23s 726ms/step - loss: 0.3216 - acc: 0.8400 - val_loss: 0.3534 - val_acc: 0.8262\n",
      "Epoch 29/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1774 - acc: 0.92 - ETA: 15s - loss: 0.2071 - acc: 0.92 - ETA: 15s - loss: 0.2735 - acc: 0.86 - ETA: 14s - loss: 0.3162 - acc: 0.84 - ETA: 14s - loss: 0.3033 - acc: 0.85 - ETA: 13s - loss: 0.3140 - acc: 0.85 - ETA: 13s - loss: 0.3052 - acc: 0.85 - ETA: 12s - loss: 0.3141 - acc: 0.84 - ETA: 12s - loss: 0.3187 - acc: 0.84 - ETA: 11s - loss: 0.3084 - acc: 0.84 - ETA: 11s - loss: 0.3069 - acc: 0.85 - ETA: 10s - loss: 0.3128 - acc: 0.84 - ETA: 9s - loss: 0.3251 - acc: 0.8433 - ETA: 9s - loss: 0.3288 - acc: 0.838 - ETA: 8s - loss: 0.3311 - acc: 0.836 - ETA: 8s - loss: 0.3259 - acc: 0.841 - ETA: 7s - loss: 0.3239 - acc: 0.842 - ETA: 7s - loss: 0.3236 - acc: 0.844 - ETA: 6s - loss: 0.3241 - acc: 0.843 - ETA: 6s - loss: 0.3250 - acc: 0.842 - ETA: 5s - loss: 0.3334 - acc: 0.838 - ETA: 5s - loss: 0.3383 - acc: 0.835 - ETA: 4s - loss: 0.3448 - acc: 0.830 - ETA: 4s - loss: 0.3434 - acc: 0.831 - ETA: 3s - loss: 0.3443 - acc: 0.831 - ETA: 3s - loss: 0.3451 - acc: 0.830 - ETA: 2s - loss: 0.3459 - acc: 0.829 - ETA: 2s - loss: 0.3418 - acc: 0.832 - ETA: 1s - loss: 0.3412 - acc: 0.830 - ETA: 1s - loss: 0.3364 - acc: 0.833 - ETA: 0s - loss: 0.3311 - acc: 0.837 - 23s 726ms/step - loss: 0.3295 - acc: 0.8391 - val_loss: 0.7975 - val_acc: 0.4995\n",
      "Epoch 30/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1718 - acc: 0.93 - ETA: 15s - loss: 0.2205 - acc: 0.91 - ETA: 15s - loss: 0.3354 - acc: 0.83 - ETA: 14s - loss: 0.4080 - acc: 0.79 - ETA: 14s - loss: 0.3766 - acc: 0.81 - ETA: 13s - loss: 0.3862 - acc: 0.81 - ETA: 13s - loss: 0.3745 - acc: 0.81 - ETA: 12s - loss: 0.3786 - acc: 0.81 - ETA: 12s - loss: 0.3828 - acc: 0.80 - ETA: 11s - loss: 0.3701 - acc: 0.81 - ETA: 11s - loss: 0.3647 - acc: 0.81 - ETA: 10s - loss: 0.3681 - acc: 0.81 - ETA: 10s - loss: 0.3847 - acc: 0.80 - ETA: 9s - loss: 0.3834 - acc: 0.8068 - ETA: 8s - loss: 0.3842 - acc: 0.804 - ETA: 8s - loss: 0.3781 - acc: 0.811 - ETA: 7s - loss: 0.3739 - acc: 0.813 - ETA: 7s - loss: 0.3717 - acc: 0.816 - ETA: 6s - loss: 0.3704 - acc: 0.815 - ETA: 6s - loss: 0.3694 - acc: 0.814 - ETA: 5s - loss: 0.3730 - acc: 0.811 - ETA: 5s - loss: 0.3748 - acc: 0.811 - ETA: 4s - loss: 0.3763 - acc: 0.809 - ETA: 4s - loss: 0.3716 - acc: 0.812 - ETA: 3s - loss: 0.3699 - acc: 0.814 - ETA: 3s - loss: 0.3689 - acc: 0.815 - ETA: 2s - loss: 0.3686 - acc: 0.815 - ETA: 2s - loss: 0.3625 - acc: 0.819 - ETA: 1s - loss: 0.3612 - acc: 0.819 - ETA: 1s - loss: 0.3571 - acc: 0.821 - ETA: 0s - loss: 0.3522 - acc: 0.823 - 23s 726ms/step - loss: 0.3510 - acc: 0.8249 - val_loss: 2.7483 - val_acc: 0.7099\n",
      "Epoch 31/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 16s - loss: 0.1787 - acc: 0.93 - ETA: 15s - loss: 0.2381 - acc: 0.91 - ETA: 15s - loss: 0.3052 - acc: 0.84 - ETA: 14s - loss: 0.3537 - acc: 0.82 - ETA: 14s - loss: 0.3386 - acc: 0.83 - ETA: 13s - loss: 0.3526 - acc: 0.82 - ETA: 13s - loss: 0.3420 - acc: 0.83 - ETA: 12s - loss: 0.3467 - acc: 0.82 - ETA: 12s - loss: 0.3489 - acc: 0.82 - ETA: 11s - loss: 0.3363 - acc: 0.82 - ETA: 11s - loss: 0.3330 - acc: 0.83 - ETA: 10s - loss: 0.3380 - acc: 0.82 - ETA: 10s - loss: 0.3527 - acc: 0.82 - ETA: 9s - loss: 0.3532 - acc: 0.8233 - ETA: 8s - loss: 0.3547 - acc: 0.820 - ETA: 8s - loss: 0.3487 - acc: 0.827 - ETA: 7s - loss: 0.3450 - acc: 0.829 - ETA: 7s - loss: 0.3443 - acc: 0.830 - ETA: 6s - loss: 0.3444 - acc: 0.829 - ETA: 6s - loss: 0.3450 - acc: 0.828 - ETA: 5s - loss: 0.3460 - acc: 0.825 - ETA: 5s - loss: 0.3463 - acc: 0.825 - ETA: 4s - loss: 0.3459 - acc: 0.825 - ETA: 4s - loss: 0.3412 - acc: 0.828 - ETA: 3s - loss: 0.3387 - acc: 0.830 - ETA: 3s - loss: 0.3375 - acc: 0.831 - ETA: 2s - loss: 0.3373 - acc: 0.831 - ETA: 2s - loss: 0.3316 - acc: 0.834 - ETA: 1s - loss: 0.3308 - acc: 0.834 - ETA: 1s - loss: 0.3283 - acc: 0.835 - ETA: 0s - loss: 0.3233 - acc: 0.838 - 23s 727ms/step - loss: 0.3226 - acc: 0.8397 - val_loss: 0.8105 - val_acc: 0.7933\n",
      "Epoch 32/32\n",
      "32/32 [==============================] - ETA: 16s - loss: 0.1645 - acc: 0.94 - ETA: 15s - loss: 0.1999 - acc: 0.93 - ETA: 15s - loss: 0.2731 - acc: 0.87 - ETA: 14s - loss: 0.3166 - acc: 0.84 - ETA: 14s - loss: 0.2994 - acc: 0.86 - ETA: 13s - loss: 0.3132 - acc: 0.85 - ETA: 13s - loss: 0.3043 - acc: 0.85 - ETA: 12s - loss: 0.3134 - acc: 0.85 - ETA: 12s - loss: 0.3171 - acc: 0.84 - ETA: 11s - loss: 0.3063 - acc: 0.85 - ETA: 11s - loss: 0.3046 - acc: 0.85 - ETA: 10s - loss: 0.3101 - acc: 0.84 - ETA: 10s - loss: 0.3233 - acc: 0.84 - ETA: 9s - loss: 0.3268 - acc: 0.8403 - ETA: 8s - loss: 0.3292 - acc: 0.838 - ETA: 8s - loss: 0.3235 - acc: 0.844 - ETA: 7s - loss: 0.3209 - acc: 0.845 - ETA: 7s - loss: 0.3210 - acc: 0.846 - ETA: 6s - loss: 0.3225 - acc: 0.843 - ETA: 6s - loss: 0.3233 - acc: 0.842 - ETA: 5s - loss: 0.3249 - acc: 0.838 - ETA: 5s - loss: 0.3260 - acc: 0.838 - ETA: 4s - loss: 0.3270 - acc: 0.837 - ETA: 4s - loss: 0.3231 - acc: 0.839 - ETA: 3s - loss: 0.3217 - acc: 0.841 - ETA: 3s - loss: 0.3216 - acc: 0.841 - ETA: 2s - loss: 0.3211 - acc: 0.840 - ETA: 2s - loss: 0.3160 - acc: 0.843 - ETA: 1s - loss: 0.3164 - acc: 0.843 - ETA: 1s - loss: 0.3122 - acc: 0.846 - ETA: 0s - loss: 0.3096 - acc: 0.848 - 23s 726ms/step - loss: 0.3094 - acc: 0.8488 - val_loss: 0.4567 - val_acc: 0.7636\n"
     ]
    }
   ],
   "source": [
    "%run segnet_train-gf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n",
      "src: (3, 2048, 2560) (2048, 2560) 2048 2560\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "pre: (2048, 2560) {0, 1}\n",
      "src: (3, 2816, 2048) (2816, 2048) 2816 2048\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "pre: (2816, 2048) {0, 1}\n"
     ]
    }
   ],
   "source": [
    "%run segnet_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
