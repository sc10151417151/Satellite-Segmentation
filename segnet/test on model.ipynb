{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#coding=utf-8\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import numpy as np \n",
    "from keras import *\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from keras.preprocessing.image import img_to_array  \n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from Models.utils import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "from keras import backend as K \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "import gdal\n",
    "seed = 7  \n",
    "np.random.seed(seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for training  \n",
    "def generateData(batch_size,img_w,img_h,n_label,image_names=[],label_names=[]): \n",
    "    print ('gen-Sub-Image-Data...')\n",
    "    image_filepath ='D:\\Python\\seg-data\\data_MB/'\n",
    "    batch_num=0\n",
    "    while True:   \n",
    "        bs=batch_size\n",
    "        \n",
    "        dataset = gdal.Open(image_filepath+image_names[batch_num%len(image_names)])\n",
    "        im_width = dataset.RasterXSize #栅格矩阵的列数\n",
    "        im_height = dataset.RasterYSize #栅格矩阵的行数\n",
    "        image_data = dataset.ReadAsArray(0,0,im_width,im_height)\n",
    "        label_data=cv2.imread(image_filepath+label_names[batch_num%len(image_names)],cv2.IMREAD_GRAYSCALE)\n",
    "        train_data = []  \n",
    "        train_label =  []  \n",
    "        for i in range(bs):\n",
    "            random_width = random.randint(0, im_width - img_w - 1)\n",
    "            random_height = random.randint(0, im_height - img_h - 1)\n",
    "            bands_roi=[]\n",
    "            for band in image_data :\n",
    "                band_roi = band[random_height: random_height + img_h, random_width: random_width + img_w]\n",
    "                bands_roi.append(band_roi)\n",
    "            data_roi=bands_roi\n",
    "            #to_categorical(train_label, num_classes=n_label)  \n",
    "            label_roi = to_categorical((label_data[random_height: random_height + img_h, random_width: random_width + img_w]).flatten(), num_classes=n_label)\n",
    "            train_data.append( data_roi)  \n",
    "            train_label.append(label_roi)\n",
    "        #yield(np.array(train_data).shape,np.array(train_label).shape)    \n",
    "        yield(np.array(train_data),np.array(train_label))\n",
    "        batch_num=batch_num+1\n",
    "#image_names_set=['test.tif']\n",
    "#label_names_set=['test_label.png']\n",
    "#for i in(generateData(8,256,256,2,image_names_set,label_names_set)):\n",
    "#    print(i)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataTF(batch_size,img_w,img_h,n_label,image_names=[],label_names=[]): \n",
    "    print ('gen-Sub-Image-Data...')\n",
    "    image_filepath ='D:\\Python\\seg-data\\data_MB/'\n",
    "    batch_num=0\n",
    "    while True:   \n",
    "        bs=batch_size\n",
    "        \n",
    "        dataset = gdal.Open(image_filepath+image_names[batch_num%len(image_names)])\n",
    "        im_width = dataset.RasterXSize #栅格矩阵的列数\n",
    "        im_height = dataset.RasterYSize #栅格矩阵的行数\n",
    "        image_data = dataset.ReadAsArray(0,0,im_width,im_height)\n",
    "        label_data=cv2.imread(image_filepath+label_names[batch_num%len(image_names)],cv2.IMREAD_GRAYSCALE)\n",
    "        train_data = []  \n",
    "        train_label =  []  \n",
    "        for i in range(bs):\n",
    "            random_width = random.randint(0, im_width - img_w - 1)\n",
    "            random_height = random.randint(0, im_height - img_h - 1)\n",
    "            bands_roi=[]\n",
    "            for band in image_data :\n",
    "                band_roi = band[random_height: random_height + img_h, random_width: random_width + img_w]\n",
    "                bands_roi.append(band_roi)\n",
    "                \n",
    "            data_roi=cv2.merge(bands_roi)\n",
    "            #to_categorical(train_label, num_classes=n_label)  \n",
    "            label_roi = to_categorical((label_data[random_height: random_height + img_h, random_width: random_width + img_w]).flatten(), num_classes=n_label)\n",
    "            train_data.append( data_roi)  \n",
    "            train_label.append(label_roi)\n",
    "        #yield(np.array(train_data).shape,np.array(train_label).shape)    \n",
    "        yield(np.array(train_data),np.array(train_label))\n",
    "        batch_num=batch_num+1\n",
    "#image_names_set=['test.tif']\n",
    "#label_names_set=['test_label.png']\n",
    "#for i in(generateDataTF(8,256,256,2,image_names_set,label_names_set)):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SegNet(\n",
    "        input_shape=(256,256,4),\n",
    "        n_labels=2,\n",
    "        kernel=3,\n",
    "        pool_size=(2, 2),\n",
    "        output_mode=\"softmax\"):\n",
    "    # encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    img_w=input_shape[1]\n",
    "    img_h=input_shape[2] \n",
    "    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\")(inputs)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = Activation(\"relu\")(conv_1)\n",
    "    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    conv_2 = Activation(\"relu\")(conv_2)\n",
    "\n",
    "    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size)(conv_2)\n",
    "\n",
    "    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\")(pool_1)\n",
    "    conv_3 = BatchNormalization()(conv_3)\n",
    "    conv_3 = Activation(\"relu\")(conv_3)\n",
    "    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_3)\n",
    "    conv_4 = BatchNormalization()(conv_4)\n",
    "    conv_4 = Activation(\"relu\")(conv_4)\n",
    "\n",
    "    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size)(conv_4)\n",
    "\n",
    "    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\")(pool_2)\n",
    "    conv_5 = BatchNormalization()(conv_5)\n",
    "    conv_5 = Activation(\"relu\")(conv_5)\n",
    "    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_5)\n",
    "    conv_6 = BatchNormalization()(conv_6)\n",
    "    conv_6 = Activation(\"relu\")(conv_6)\n",
    "    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_6)\n",
    "    conv_7 = BatchNormalization()(conv_7)\n",
    "    conv_7 = Activation(\"relu\")(conv_7)\n",
    "\n",
    "    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size)(conv_7)\n",
    "\n",
    "    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_3)\n",
    "    conv_8 = BatchNormalization()(conv_8)\n",
    "    conv_8 = Activation(\"relu\")(conv_8)\n",
    "    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_8)\n",
    "    conv_9 = BatchNormalization()(conv_9)\n",
    "    conv_9 = Activation(\"relu\")(conv_9)\n",
    "    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_9)\n",
    "    conv_10 = BatchNormalization()(conv_10)\n",
    "    conv_10 = Activation(\"relu\")(conv_10)\n",
    "\n",
    "    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size)(conv_10)\n",
    "\n",
    "    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_4)\n",
    "    conv_11 = BatchNormalization()(conv_11)\n",
    "    conv_11 = Activation(\"relu\")(conv_11)\n",
    "    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_11)\n",
    "    conv_12 = BatchNormalization()(conv_12)\n",
    "    conv_12 = Activation(\"relu\")(conv_12)\n",
    "    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_12)\n",
    "    conv_13 = BatchNormalization()(conv_13)\n",
    "    conv_13 = Activation(\"relu\")(conv_13)\n",
    "\n",
    "    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size)(conv_13)\n",
    "    print(\"Build enceder done..\")\n",
    "\n",
    "    # decoder\n",
    "\n",
    "    unpool_1 = MaxUnpooling2D(pool_size)([pool_5, mask_5])\n",
    "\n",
    "    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\n",
    "    conv_14 = BatchNormalization()(conv_14)\n",
    "    conv_14 = Activation(\"relu\")(conv_14)\n",
    "    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_14)\n",
    "    conv_15 = BatchNormalization()(conv_15)\n",
    "    conv_15 = Activation(\"relu\")(conv_15)\n",
    "    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_15)\n",
    "    conv_16 = BatchNormalization()(conv_16)\n",
    "    conv_16 = Activation(\"relu\")(conv_16)\n",
    "\n",
    "    unpool_2 = MaxUnpooling2D(pool_size)([conv_16, mask_4])\n",
    "\n",
    "    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_2)\n",
    "    conv_17 = BatchNormalization()(conv_17)\n",
    "    conv_17 = Activation(\"relu\")(conv_17)\n",
    "    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_17)\n",
    "    conv_18 = BatchNormalization()(conv_18)\n",
    "    conv_18 = Activation(\"relu\")(conv_18)\n",
    "    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_18)\n",
    "    conv_19 = BatchNormalization()(conv_19)\n",
    "    conv_19 = Activation(\"relu\")(conv_19)\n",
    "\n",
    "    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\n",
    "\n",
    "    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\")(unpool_3)\n",
    "    conv_20 = BatchNormalization()(conv_20)\n",
    "    conv_20 = Activation(\"relu\")(conv_20)\n",
    "    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_20)\n",
    "    conv_21 = BatchNormalization()(conv_21)\n",
    "    conv_21 = Activation(\"relu\")(conv_21)\n",
    "    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_21)\n",
    "    conv_22 = BatchNormalization()(conv_22)\n",
    "    conv_22 = Activation(\"relu\")(conv_22)\n",
    "\n",
    "    unpool_4 = MaxUnpooling2D(pool_size)([conv_22, mask_2])\n",
    "\n",
    "    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\")(unpool_4)\n",
    "    conv_23 = BatchNormalization()(conv_23)\n",
    "    conv_23 = Activation(\"relu\")(conv_23)\n",
    "    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_23)\n",
    "    conv_24 = BatchNormalization()(conv_24)\n",
    "    conv_24 = Activation(\"relu\")(conv_24)\n",
    "\n",
    "    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\n",
    "\n",
    "    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\")(unpool_5)\n",
    "    conv_25 = BatchNormalization()(conv_25)\n",
    "    conv_25 = Activation(\"relu\")(conv_25)\n",
    "\n",
    "    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"same\")(conv_25)\n",
    "    conv_26 = BatchNormalization()(conv_26)\n",
    "   \n",
    "    conv_26  = Reshape((n_labels,-1))(conv_26 )\n",
    "\n",
    "    conv_26  = Permute((2,1))(conv_26 )\n",
    "    outputs = Activation(output_mode)(conv_26)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build enceder done..\n",
      "Tensor(\"max_unpooling2d_6/max_unpooling2d_6/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_7/max_unpooling2d_7/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_8/max_unpooling2d_8/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_9/max_unpooling2d_9/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_10/max_unpooling2d_10/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "the number of train data is 1000 500\n",
      "the number of val data is 200 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=500, epochs=1, verbose=1, validation_data=<generator..., validation_steps=100, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "gen-Sub-Image-Data...gen-Sub-Image-Data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/500 [===========>..................] - ETA: 44:14 - loss: 0.8983 - acc: 0.49 - ETA: 23:06 - loss: 0.9079 - acc: 0.48 - ETA: 16:00 - loss: 0.8943 - acc: 0.48 - ETA: 12:30 - loss: 0.8932 - acc: 0.48 - ETA: 10:20 - loss: 0.8792 - acc: 0.49 - ETA: 8:55 - loss: 0.8792 - acc: 0.4932 - ETA: 7:55 - loss: 0.8783 - acc: 0.493 - ETA: 7:10 - loss: 0.8756 - acc: 0.493 - ETA: 6:34 - loss: 0.8804 - acc: 0.492 - ETA: 6:06 - loss: 0.8779 - acc: 0.493 - ETA: 5:42 - loss: 0.8719 - acc: 0.495 - ETA: 5:22 - loss: 0.8748 - acc: 0.494 - ETA: 5:06 - loss: 0.8711 - acc: 0.495 - ETA: 4:51 - loss: 0.8735 - acc: 0.494 - ETA: 4:39 - loss: 0.8687 - acc: 0.495 - ETA: 4:27 - loss: 0.8698 - acc: 0.495 - ETA: 4:18 - loss: 0.8667 - acc: 0.495 - ETA: 4:09 - loss: 0.8670 - acc: 0.495 - ETA: 4:01 - loss: 0.8662 - acc: 0.496 - ETA: 3:54 - loss: 0.8642 - acc: 0.496 - ETA: 3:48 - loss: 0.8641 - acc: 0.496 - ETA: 3:43 - loss: 0.8610 - acc: 0.497 - ETA: 3:38 - loss: 0.8581 - acc: 0.498 - ETA: 3:33 - loss: 0.8611 - acc: 0.497 - ETA: 3:29 - loss: 0.8618 - acc: 0.496 - ETA: 3:25 - loss: 0.8602 - acc: 0.497 - ETA: 3:21 - loss: 0.8571 - acc: 0.498 - ETA: 3:17 - loss: 0.8564 - acc: 0.498 - ETA: 3:14 - loss: 0.8553 - acc: 0.498 - ETA: 3:10 - loss: 0.8522 - acc: 0.499 - ETA: 3:07 - loss: 0.8500 - acc: 0.500 - ETA: 3:04 - loss: 0.8506 - acc: 0.499 - ETA: 3:02 - loss: 0.8472 - acc: 0.501 - ETA: 2:59 - loss: 0.8445 - acc: 0.501 - ETA: 2:57 - loss: 0.8422 - acc: 0.502 - ETA: 2:55 - loss: 0.8450 - acc: 0.501 - ETA: 2:53 - loss: 0.8453 - acc: 0.501 - ETA: 2:50 - loss: 0.8420 - acc: 0.502 - ETA: 2:49 - loss: 0.8455 - acc: 0.500 - ETA: 2:47 - loss: 0.8467 - acc: 0.500 - ETA: 2:45 - loss: 0.8438 - acc: 0.501 - ETA: 2:44 - loss: 0.8412 - acc: 0.502 - ETA: 2:42 - loss: 0.8403 - acc: 0.502 - ETA: 2:40 - loss: 0.8368 - acc: 0.503 - ETA: 2:39 - loss: 0.8372 - acc: 0.502 - ETA: 2:37 - loss: 0.8350 - acc: 0.504 - ETA: 2:36 - loss: 0.8321 - acc: 0.505 - ETA: 2:34 - loss: 0.8303 - acc: 0.506 - ETA: 2:33 - loss: 0.8288 - acc: 0.507 - ETA: 2:32 - loss: 0.8251 - acc: 0.509 - ETA: 2:31 - loss: 0.8234 - acc: 0.509 - ETA: 2:29 - loss: 0.8245 - acc: 0.509 - ETA: 2:28 - loss: 0.8223 - acc: 0.510 - ETA: 2:27 - loss: 0.8194 - acc: 0.512 - ETA: 2:26 - loss: 0.8213 - acc: 0.511 - ETA: 2:25 - loss: 0.8203 - acc: 0.511 - ETA: 2:24 - loss: 0.8211 - acc: 0.511 - ETA: 2:23 - loss: 0.8175 - acc: 0.513 - ETA: 2:22 - loss: 0.8139 - acc: 0.515 - ETA: 2:21 - loss: 0.8157 - acc: 0.514 - ETA: 2:20 - loss: 0.8118 - acc: 0.517 - ETA: 2:19 - loss: 0.8135 - acc: 0.516 - ETA: 2:18 - loss: 0.8095 - acc: 0.519 - ETA: 2:17 - loss: 0.8091 - acc: 0.518 - ETA: 2:16 - loss: 0.8098 - acc: 0.517 - ETA: 2:16 - loss: 0.8060 - acc: 0.520 - ETA: 2:15 - loss: 0.8089 - acc: 0.518 - ETA: 2:14 - loss: 0.8107 - acc: 0.517 - ETA: 2:13 - loss: 0.8069 - acc: 0.520 - ETA: 2:12 - loss: 0.8073 - acc: 0.520 - ETA: 2:11 - loss: 0.8056 - acc: 0.520 - ETA: 2:11 - loss: 0.8077 - acc: 0.519 - ETA: 2:10 - loss: 0.8093 - acc: 0.518 - ETA: 2:09 - loss: 0.8068 - acc: 0.519 - ETA: 2:09 - loss: 0.8075 - acc: 0.519 - ETA: 2:08 - loss: 0.8100 - acc: 0.518 - ETA: 2:08 - loss: 0.8081 - acc: 0.519 - ETA: 2:07 - loss: 0.8087 - acc: 0.518 - ETA: 2:06 - loss: 0.8076 - acc: 0.519 - ETA: 2:06 - loss: 0.8079 - acc: 0.519 - ETA: 2:05 - loss: 0.8081 - acc: 0.519 - ETA: 2:05 - loss: 0.8049 - acc: 0.521 - ETA: 2:04 - loss: 0.8029 - acc: 0.522 - ETA: 2:03 - loss: 0.8021 - acc: 0.522 - ETA: 2:03 - loss: 0.8022 - acc: 0.522 - ETA: 2:02 - loss: 0.7990 - acc: 0.525 - ETA: 2:02 - loss: 0.7966 - acc: 0.526 - ETA: 2:01 - loss: 0.7965 - acc: 0.526 - ETA: 2:00 - loss: 0.7976 - acc: 0.525 - ETA: 2:00 - loss: 0.7944 - acc: 0.528 - ETA: 1:59 - loss: 0.7945 - acc: 0.527 - ETA: 1:59 - loss: 0.7941 - acc: 0.527 - ETA: 1:58 - loss: 0.7949 - acc: 0.527 - ETA: 1:58 - loss: 0.7932 - acc: 0.527 - ETA: 1:57 - loss: 0.7914 - acc: 0.528 - ETA: 1:56 - loss: 0.7945 - acc: 0.527 - ETA: 1:56 - loss: 0.7939 - acc: 0.527 - ETA: 1:55 - loss: 0.7910 - acc: 0.529 - ETA: 1:55 - loss: 0.7892 - acc: 0.530 - ETA: 1:55 - loss: 0.7890 - acc: 0.531 - ETA: 1:54 - loss: 0.7859 - acc: 0.533 - ETA: 1:54 - loss: 0.7875 - acc: 0.533 - ETA: 1:53 - loss: 0.7875 - acc: 0.533 - ETA: 1:53 - loss: 0.7870 - acc: 0.533 - ETA: 1:52 - loss: 0.7840 - acc: 0.536 - ETA: 1:52 - loss: 0.7821 - acc: 0.537 - ETA: 1:51 - loss: 0.7813 - acc: 0.537 - ETA: 1:51 - loss: 0.7804 - acc: 0.538 - ETA: 1:51 - loss: 0.7787 - acc: 0.539 - ETA: 1:50 - loss: 0.7757 - acc: 0.541 - ETA: 1:50 - loss: 0.7728 - acc: 0.544 - ETA: 1:49 - loss: 0.7749 - acc: 0.543 - ETA: 1:49 - loss: 0.7744 - acc: 0.543 - ETA: 1:48 - loss: 0.7724 - acc: 0.544 - ETA: 1:48 - loss: 0.7737 - acc: 0.543 - ETA: 1:47 - loss: 0.7725 - acc: 0.544 - ETA: 1:47 - loss: 0.7743 - acc: 0.543 - ETA: 1:46 - loss: 0.7715 - acc: 0.545 - ETA: 1:46 - loss: 0.7712 - acc: 0.545 - ETA: 1:46 - loss: 0.7735 - acc: 0.544 - ETA: 1:45 - loss: 0.7740 - acc: 0.543 - ETA: 1:45 - loss: 0.7751 - acc: 0.543 - ETA: 1:44 - loss: 0.7770 - acc: 0.542 - ETA: 1:44 - loss: 0.7771 - acc: 0.541 - ETA: 1:44 - loss: 0.7746 - acc: 0.544 - ETA: 1:43 - loss: 0.7748 - acc: 0.544 - ETA: 1:43 - loss: 0.7744 - acc: 0.544 - ETA: 1:42 - loss: 0.7718 - acc: 0.546 - ETA: 1:42 - loss: 0.7733 - acc: 0.546 - ETA: 1:42 - loss: 0.7735 - acc: 0.545 - ETA: 1:41 - loss: 0.7752 - acc: 0.544 - ETA: 1:41 - loss: 0.7776 - acc: 0.543 - ETA: 1:41 - loss: 0.7751 - acc: 0.545 - ETA: 1:40 - loss: 0.7740 - acc: 0.546 - ETA: 1:40 - loss: 0.7732 - acc: 0.546 - ETA: 1:39 - loss: 0.7732 - acc: 0.546 - ETA: 1:39 - loss: 0.7751 - acc: 0.545 - ETA: 1:39 - loss: 0.7764 - acc: 0.544 - ETA: 1:38 - loss: 0.7753 - acc: 0.545 - ETA: 1:38 - loss: 0.7753 - acc: 0.545 - ETA: 1:38 - loss: 0.7759 - acc: 0.545 - ETA: 1:37 - loss: 0.7760 - acc: 0.545 - ETA: 1:37 - loss: 0.7753 - acc: 0.546 - ETA: 1:37 - loss: 0.7747 - acc: 0.546 - ETA: 1:36 - loss: 0.7751 - acc: 0.546 - ETA: 1:36 - loss: 0.7730 - acc: 0.548 - ETA: 1:36 - loss: 0.7708 - acc: 0.550 - ETA: 1:35 - loss: 0.7693 - acc: 0.551 - ETA: 1:35 - loss: 0.7681 - acc: 0.552 - ETA: 1:34 - loss: 0.7695 - acc: 0.552 - ETA: 1:34 - loss: 0.7696 - acc: 0.551 - ETA: 1:34 - loss: 0.7674 - acc: 0.553 - ETA: 1:33 - loss: 0.7665 - acc: 0.554 - ETA: 1:33 - loss: 0.7652 - acc: 0.555 - ETA: 1:33 - loss: 0.7639 - acc: 0.556 - ETA: 1:32 - loss: 0.7640 - acc: 0.555 - ETA: 1:32 - loss: 0.7635 - acc: 0.556 - ETA: 1:32 - loss: 0.7625 - acc: 0.557 - ETA: 1:31 - loss: 0.7615 - acc: 0.558 - ETA: 1:31 - loss: 0.7603 - acc: 0.558 - ETA: 1:31 - loss: 0.7590 - acc: 0.559 - ETA: 1:30 - loss: 0.7568 - acc: 0.561 - ETA: 1:30 - loss: 0.7554 - acc: 0.562 - ETA: 1:30 - loss: 0.7546 - acc: 0.563 - ETA: 1:29 - loss: 0.7532 - acc: 0.564 - ETA: 1:29 - loss: 0.7514 - acc: 0.566 - ETA: 1:29 - loss: 0.7495 - acc: 0.567 - ETA: 1:28 - loss: 0.7482 - acc: 0.568 - ETA: 1:28 - loss: 0.7470 - acc: 0.569 - ETA: 1:28 - loss: 0.7491 - acc: 0.568 - ETA: 1:28 - loss: 0.7483 - acc: 0.569 - ETA: 1:27 - loss: 0.7487 - acc: 0.568 - ETA: 1:27 - loss: 0.7477 - acc: 0.569 - ETA: 1:27 - loss: 0.7462 - acc: 0.570 - ETA: 1:26 - loss: 0.7451 - acc: 0.571 - ETA: 1:26 - loss: 0.7437 - acc: 0.572 - ETA: 1:26 - loss: 0.7423 - acc: 0.573 - ETA: 1:25 - loss: 0.7402 - acc: 0.575 - ETA: 1:25 - loss: 0.7387 - acc: 0.577 - ETA: 1:25 - loss: 0.7375 - acc: 0.577 - ETA: 1:24 - loss: 0.7364 - acc: 0.578 - ETA: 1:24 - loss: 0.7355 - acc: 0.579 - ETA: 1:24 - loss: 0.7343 - acc: 0.579 - ETA: 1:23 - loss: 0.7323 - acc: 0.581 - ETA: 1:23 - loss: 0.7302 - acc: 0.583 - ETA: 1:23 - loss: 0.7292 - acc: 0.585 - ETA: 1:23 - loss: 0.7280 - acc: 0.585 - ETA: 1:22 - loss: 0.7265 - acc: 0.587 - ETA: 1:22 - loss: 0.7267 - acc: 0.587 - ETA: 1:22 - loss: 0.7257 - acc: 0.588 - ETA: 1:21 - loss: 0.7245 - acc: 0.588 - ETA: 1:21 - loss: 0.7229 - acc: 0.590 - ETA: 1:21 - loss: 0.7220 - acc: 0.590 - ETA: 1:20 - loss: 0.7213 - acc: 0.591 - ETA: 1:20 - loss: 0.7205 - acc: 0.591 - ETA: 1:20 - loss: 0.7193 - acc: 0.592 - ETA: 1:19 - loss: 0.7176 - acc: 0.593 - ETA: 1:19 - loss: 0.7165 - acc: 0.594 - ETA: 1:19 - loss: 0.7161 - acc: 0.595 - ETA: 1:19 - loss: 0.7148 - acc: 0.596 - ETA: 1:18 - loss: 0.7139 - acc: 0.596 - ETA: 1:18 - loss: 0.7137 - acc: 0.597 - ETA: 1:18 - loss: 0.7129 - acc: 0.5980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415/500 [=======================>......] - ETA: 1:17 - loss: 0.7121 - acc: 0.598 - ETA: 1:17 - loss: 0.7123 - acc: 0.598 - ETA: 1:17 - loss: 0.7114 - acc: 0.599 - ETA: 1:17 - loss: 0.7095 - acc: 0.601 - ETA: 1:16 - loss: 0.7076 - acc: 0.602 - ETA: 1:16 - loss: 0.7063 - acc: 0.604 - ETA: 1:16 - loss: 0.7053 - acc: 0.604 - ETA: 1:15 - loss: 0.7037 - acc: 0.606 - ETA: 1:15 - loss: 0.7026 - acc: 0.606 - ETA: 1:15 - loss: 0.7011 - acc: 0.608 - ETA: 1:15 - loss: 0.6996 - acc: 0.609 - ETA: 1:14 - loss: 0.7000 - acc: 0.609 - ETA: 1:14 - loss: 0.6987 - acc: 0.610 - ETA: 1:14 - loss: 0.6973 - acc: 0.611 - ETA: 1:13 - loss: 0.6962 - acc: 0.612 - ETA: 1:13 - loss: 0.6946 - acc: 0.613 - ETA: 1:13 - loss: 0.6933 - acc: 0.614 - ETA: 1:12 - loss: 0.6920 - acc: 0.616 - ETA: 1:12 - loss: 0.6907 - acc: 0.617 - ETA: 1:12 - loss: 0.6894 - acc: 0.618 - ETA: 1:12 - loss: 0.6896 - acc: 0.618 - ETA: 1:11 - loss: 0.6878 - acc: 0.620 - ETA: 1:11 - loss: 0.6870 - acc: 0.620 - ETA: 1:11 - loss: 0.6856 - acc: 0.621 - ETA: 1:10 - loss: 0.6843 - acc: 0.622 - ETA: 1:10 - loss: 0.6849 - acc: 0.622 - ETA: 1:10 - loss: 0.6837 - acc: 0.622 - ETA: 1:10 - loss: 0.6822 - acc: 0.624 - ETA: 1:09 - loss: 0.6824 - acc: 0.624 - ETA: 1:09 - loss: 0.6815 - acc: 0.625 - ETA: 1:09 - loss: 0.6821 - acc: 0.624 - ETA: 1:08 - loss: 0.6808 - acc: 0.625 - ETA: 1:08 - loss: 0.6797 - acc: 0.627 - ETA: 1:08 - loss: 0.6786 - acc: 0.628 - ETA: 1:08 - loss: 0.6780 - acc: 0.628 - ETA: 1:07 - loss: 0.6768 - acc: 0.629 - ETA: 1:07 - loss: 0.6760 - acc: 0.629 - ETA: 1:07 - loss: 0.6751 - acc: 0.630 - ETA: 1:07 - loss: 0.6736 - acc: 0.631 - ETA: 1:06 - loss: 0.6719 - acc: 0.633 - ETA: 1:06 - loss: 0.6710 - acc: 0.633 - ETA: 1:06 - loss: 0.6707 - acc: 0.634 - ETA: 1:05 - loss: 0.6691 - acc: 0.635 - ETA: 1:05 - loss: 0.6674 - acc: 0.637 - ETA: 1:05 - loss: 0.6657 - acc: 0.638 - ETA: 1:05 - loss: 0.6649 - acc: 0.639 - ETA: 1:04 - loss: 0.6633 - acc: 0.640 - ETA: 1:04 - loss: 0.6625 - acc: 0.641 - ETA: 1:04 - loss: 0.6609 - acc: 0.642 - ETA: 1:03 - loss: 0.6617 - acc: 0.642 - ETA: 1:03 - loss: 0.6614 - acc: 0.642 - ETA: 1:03 - loss: 0.6605 - acc: 0.643 - ETA: 1:03 - loss: 0.6589 - acc: 0.644 - ETA: 1:02 - loss: 0.6582 - acc: 0.644 - ETA: 1:02 - loss: 0.6573 - acc: 0.645 - ETA: 1:02 - loss: 0.6562 - acc: 0.646 - ETA: 1:02 - loss: 0.6561 - acc: 0.646 - ETA: 1:01 - loss: 0.6553 - acc: 0.646 - ETA: 1:01 - loss: 0.6546 - acc: 0.646 - ETA: 1:01 - loss: 0.6530 - acc: 0.648 - ETA: 1:00 - loss: 0.6521 - acc: 0.648 - ETA: 1:00 - loss: 0.6513 - acc: 0.649 - ETA: 1:00 - loss: 0.6507 - acc: 0.649 - ETA: 1:00 - loss: 0.6501 - acc: 0.649 - ETA: 59s - loss: 0.6492 - acc: 0.650 - ETA: 59s - loss: 0.6485 - acc: 0.65 - ETA: 59s - loss: 0.6477 - acc: 0.65 - ETA: 59s - loss: 0.6468 - acc: 0.65 - ETA: 58s - loss: 0.6458 - acc: 0.65 - ETA: 58s - loss: 0.6448 - acc: 0.65 - ETA: 58s - loss: 0.6440 - acc: 0.65 - ETA: 57s - loss: 0.6441 - acc: 0.65 - ETA: 57s - loss: 0.6426 - acc: 0.65 - ETA: 57s - loss: 0.6425 - acc: 0.65 - ETA: 57s - loss: 0.6412 - acc: 0.65 - ETA: 56s - loss: 0.6405 - acc: 0.65 - ETA: 56s - loss: 0.6393 - acc: 0.65 - ETA: 56s - loss: 0.6378 - acc: 0.65 - ETA: 56s - loss: 0.6389 - acc: 0.65 - ETA: 55s - loss: 0.6374 - acc: 0.65 - ETA: 55s - loss: 0.6364 - acc: 0.66 - ETA: 55s - loss: 0.6356 - acc: 0.66 - ETA: 54s - loss: 0.6344 - acc: 0.66 - ETA: 54s - loss: 0.6348 - acc: 0.66 - ETA: 54s - loss: 0.6344 - acc: 0.66 - ETA: 54s - loss: 0.6342 - acc: 0.66 - ETA: 53s - loss: 0.6345 - acc: 0.66 - ETA: 53s - loss: 0.6337 - acc: 0.66 - ETA: 53s - loss: 0.6324 - acc: 0.66 - ETA: 53s - loss: 0.6314 - acc: 0.66 - ETA: 52s - loss: 0.6301 - acc: 0.66 - ETA: 52s - loss: 0.6289 - acc: 0.66 - ETA: 52s - loss: 0.6294 - acc: 0.66 - ETA: 52s - loss: 0.6280 - acc: 0.66 - ETA: 51s - loss: 0.6272 - acc: 0.66 - ETA: 51s - loss: 0.6259 - acc: 0.66 - ETA: 51s - loss: 0.6248 - acc: 0.66 - ETA: 50s - loss: 0.6235 - acc: 0.67 - ETA: 50s - loss: 0.6234 - acc: 0.67 - ETA: 50s - loss: 0.6241 - acc: 0.67 - ETA: 50s - loss: 0.6230 - acc: 0.67 - ETA: 49s - loss: 0.6223 - acc: 0.67 - ETA: 49s - loss: 0.6221 - acc: 0.67 - ETA: 49s - loss: 0.6211 - acc: 0.67 - ETA: 49s - loss: 0.6205 - acc: 0.67 - ETA: 48s - loss: 0.6203 - acc: 0.67 - ETA: 48s - loss: 0.6195 - acc: 0.67 - ETA: 48s - loss: 0.6182 - acc: 0.67 - ETA: 48s - loss: 0.6196 - acc: 0.67 - ETA: 47s - loss: 0.6192 - acc: 0.67 - ETA: 47s - loss: 0.6179 - acc: 0.67 - ETA: 47s - loss: 0.6166 - acc: 0.67 - ETA: 46s - loss: 0.6174 - acc: 0.67 - ETA: 46s - loss: 0.6165 - acc: 0.67 - ETA: 46s - loss: 0.6158 - acc: 0.67 - ETA: 46s - loss: 0.6152 - acc: 0.67 - ETA: 45s - loss: 0.6141 - acc: 0.67 - ETA: 45s - loss: 0.6131 - acc: 0.68 - ETA: 45s - loss: 0.6129 - acc: 0.68 - ETA: 45s - loss: 0.6119 - acc: 0.68 - ETA: 44s - loss: 0.6111 - acc: 0.68 - ETA: 44s - loss: 0.6101 - acc: 0.68 - ETA: 44s - loss: 0.6095 - acc: 0.68 - ETA: 44s - loss: 0.6083 - acc: 0.68 - ETA: 43s - loss: 0.6072 - acc: 0.68 - ETA: 43s - loss: 0.6062 - acc: 0.68 - ETA: 43s - loss: 0.6059 - acc: 0.68 - ETA: 43s - loss: 0.6060 - acc: 0.68 - ETA: 42s - loss: 0.6050 - acc: 0.68 - ETA: 42s - loss: 0.6042 - acc: 0.68 - ETA: 42s - loss: 0.6037 - acc: 0.68 - ETA: 41s - loss: 0.6030 - acc: 0.68 - ETA: 41s - loss: 0.6022 - acc: 0.68 - ETA: 41s - loss: 0.6010 - acc: 0.69 - ETA: 41s - loss: 0.5998 - acc: 0.69 - ETA: 40s - loss: 0.5996 - acc: 0.69 - ETA: 40s - loss: 0.5985 - acc: 0.69 - ETA: 40s - loss: 0.5983 - acc: 0.69 - ETA: 40s - loss: 0.5979 - acc: 0.69 - ETA: 39s - loss: 0.5969 - acc: 0.69 - ETA: 39s - loss: 0.5965 - acc: 0.69 - ETA: 39s - loss: 0.5955 - acc: 0.69 - ETA: 39s - loss: 0.5952 - acc: 0.69 - ETA: 38s - loss: 0.5941 - acc: 0.69 - ETA: 38s - loss: 0.5935 - acc: 0.69 - ETA: 38s - loss: 0.5923 - acc: 0.69 - ETA: 38s - loss: 0.5913 - acc: 0.69 - ETA: 37s - loss: 0.5902 - acc: 0.69 - ETA: 37s - loss: 0.5892 - acc: 0.69 - ETA: 37s - loss: 0.5884 - acc: 0.69 - ETA: 37s - loss: 0.5879 - acc: 0.69 - ETA: 36s - loss: 0.5876 - acc: 0.69 - ETA: 36s - loss: 0.5869 - acc: 0.70 - ETA: 36s - loss: 0.5864 - acc: 0.70 - ETA: 36s - loss: 0.5856 - acc: 0.70 - ETA: 35s - loss: 0.5856 - acc: 0.70 - ETA: 35s - loss: 0.5847 - acc: 0.70 - ETA: 35s - loss: 0.5839 - acc: 0.70 - ETA: 34s - loss: 0.5829 - acc: 0.70 - ETA: 34s - loss: 0.5819 - acc: 0.70 - ETA: 34s - loss: 0.5811 - acc: 0.70 - ETA: 34s - loss: 0.5802 - acc: 0.70 - ETA: 33s - loss: 0.5791 - acc: 0.70 - ETA: 33s - loss: 0.5797 - acc: 0.70 - ETA: 33s - loss: 0.5800 - acc: 0.70 - ETA: 33s - loss: 0.5791 - acc: 0.70 - ETA: 32s - loss: 0.5790 - acc: 0.70 - ETA: 32s - loss: 0.5782 - acc: 0.70 - ETA: 32s - loss: 0.5774 - acc: 0.70 - ETA: 32s - loss: 0.5766 - acc: 0.70 - ETA: 31s - loss: 0.5762 - acc: 0.70 - ETA: 31s - loss: 0.5763 - acc: 0.70 - ETA: 31s - loss: 0.5759 - acc: 0.70 - ETA: 31s - loss: 0.5748 - acc: 0.70 - ETA: 30s - loss: 0.5740 - acc: 0.71 - ETA: 30s - loss: 0.5733 - acc: 0.71 - ETA: 30s - loss: 0.5723 - acc: 0.71 - ETA: 30s - loss: 0.5713 - acc: 0.71 - ETA: 29s - loss: 0.5706 - acc: 0.71 - ETA: 29s - loss: 0.5696 - acc: 0.71 - ETA: 29s - loss: 0.5687 - acc: 0.71 - ETA: 29s - loss: 0.5683 - acc: 0.71 - ETA: 28s - loss: 0.5673 - acc: 0.71 - ETA: 28s - loss: 0.5674 - acc: 0.71 - ETA: 28s - loss: 0.5667 - acc: 0.71 - ETA: 27s - loss: 0.5668 - acc: 0.71 - ETA: 27s - loss: 0.5660 - acc: 0.71 - ETA: 27s - loss: 0.5651 - acc: 0.71 - ETA: 27s - loss: 0.5649 - acc: 0.71 - ETA: 26s - loss: 0.5640 - acc: 0.71 - ETA: 26s - loss: 0.5647 - acc: 0.71 - ETA: 26s - loss: 0.5644 - acc: 0.71 - ETA: 26s - loss: 0.5642 - acc: 0.71 - ETA: 25s - loss: 0.5634 - acc: 0.71 - ETA: 25s - loss: 0.5626 - acc: 0.71 - ETA: 25s - loss: 0.5616 - acc: 0.72 - ETA: 25s - loss: 0.5614 - acc: 0.71 - ETA: 24s - loss: 0.5604 - acc: 0.72 - ETA: 24s - loss: 0.5599 - acc: 0.72 - ETA: 24s - loss: 0.5589 - acc: 0.72 - ETA: 24s - loss: 0.5580 - acc: 0.72 - ETA: 23s - loss: 0.5574 - acc: 0.72 - ETA: 23s - loss: 0.5585 - acc: 0.72 - ETA: 23s - loss: 0.5581 - acc: 0.72 - ETA: 23s - loss: 0.5571 - acc: 0.72 - ETA: 22s - loss: 0.5562 - acc: 0.72 - ETA: 22s - loss: 0.5561 - acc: 0.72 - ETA: 22s - loss: 0.5557 - acc: 0.72 - ETA: 22s - loss: 0.5549 - acc: 0.72 - ETA: 21s - loss: 0.5545 - acc: 0.72 - ETA: 21s - loss: 0.5537 - acc: 0.72 - ETA: 21s - loss: 0.5541 - acc: 0.7253"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/500 [==========================>...] - ETA: 21s - loss: 0.5533 - acc: 0.72 - ETA: 20s - loss: 0.5524 - acc: 0.72 - ETA: 20s - loss: 0.5515 - acc: 0.72 - ETA: 20s - loss: 0.5518 - acc: 0.72 - ETA: 20s - loss: 0.5515 - acc: 0.72 - ETA: 19s - loss: 0.5506 - acc: 0.72 - ETA: 19s - loss: 0.5499 - acc: 0.72 - ETA: 19s - loss: 0.5490 - acc: 0.72 - ETA: 19s - loss: 0.5485 - acc: 0.72 - ETA: 18s - loss: 0.5477 - acc: 0.72 - ETA: 18s - loss: 0.5468 - acc: 0.73 - ETA: 18s - loss: 0.5462 - acc: 0.73 - ETA: 18s - loss: 0.5460 - acc: 0.73 - ETA: 17s - loss: 0.5454 - acc: 0.73 - ETA: 17s - loss: 0.5445 - acc: 0.73 - ETA: 17s - loss: 0.5440 - acc: 0.73 - ETA: 17s - loss: 0.5447 - acc: 0.73 - ETA: 16s - loss: 0.5446 - acc: 0.73 - ETA: 16s - loss: 0.5439 - acc: 0.73 - ETA: 16s - loss: 0.5434 - acc: 0.73 - ETA: 16s - loss: 0.5425 - acc: 0.73 - ETA: 15s - loss: 0.5418 - acc: 0.73 - ETA: 15s - loss: 0.5414 - acc: 0.73 - ETA: 15s - loss: 0.5409 - acc: 0.73 - ETA: 15s - loss: 0.5400 - acc: 0.73 - ETA: 14s - loss: 0.5397 - acc: 0.73 - ETA: 14s - loss: 0.5390 - acc: 0.73 - ETA: 14s - loss: 0.5382 - acc: 0.73 - ETA: 14s - loss: 0.5378 - acc: 0.73 - ETA: 13s - loss: 0.5375 - acc: 0.73 - ETA: 13s - loss: 0.5370 - acc: 0.73 - ETA: 13s - loss: 0.5366 - acc: 0.73 - ETA: 13s - loss: 0.5360 - acc: 0.73 - ETA: 12s - loss: 0.5363 - acc: 0.73 - ETA: 12s - loss: 0.5358 - acc: 0.7386"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dde743692d64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-dde743692d64>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_numb\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mBS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                             max_q_size=1)  \n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# plot the training loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2719\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2721\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2693\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(): \n",
    "    EPOCHS = 1\n",
    "    BS = 2\n",
    "    img_w = 256  \n",
    "    img_h = 256  \n",
    "    model = SegNet() \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "    \n",
    "    modelcheck = ModelCheckpoint('..\\..\\Python\\seg-data/model/4Bands-'+time.strftime(f'%Y-%m-%d-%a-%H-%M-%S',time.localtime(time.time()))+'.h5',monitor='val_acc',save_best_only=True,mode='max')  \n",
    "    \n",
    "    callable = [modelcheck]  \n",
    "    \n",
    "    train_numb = 1000\n",
    "    valid_numb = 200\n",
    "    print (\"the number of train data is\",train_numb,train_numb//BS)  \n",
    "    print (\"the number of val data is\",valid_numb,valid_numb//BS)\n",
    "    H = model.fit_generator(generator=generateDataTF(BS,img_w,img_h,2,['test.tif'],['test_label.png']),\n",
    "                            steps_per_epoch=train_numb//BS,\n",
    "                            epochs=EPOCHS,\n",
    "                            verbose=1,\n",
    "                            validation_data=generateDataTF(BS,img_w,img_h,2,['test.tif'],['test_label.png']),\n",
    "                            validation_steps=valid_numb//BS,\n",
    "                            callbacks=callable,\n",
    "                            max_q_size=1)  \n",
    "\n",
    "    # plot the training loss and accuracy\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    N = EPOCHS\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on SegNet Satellite Seg\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(\"plot.png\")\n",
    "\n",
    "\n",
    "\n",
    "def args_parse():\n",
    "    # construct the argument parse and parse the arguments\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-a\", \"--augment\",help=\"using data augment or not\",\n",
    "                    action=\"store_true\", default=False)\n",
    "    ap.add_argument(\"-m\", \"--model\", required=True,default='model/'+time.strftime(f'%Y-%m-%d %a %H:%M:%S',time.localtime(time.time()))+'.h5'\n",
    "                    ,help=\"path to output model\")\n",
    "    ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "                    help=\"path to output accuracy/loss plot\")\n",
    "    args = vars(ap.parse_args()) \n",
    "    return args\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = SegNet()\n",
    "from keras.utils import plot_model\n",
    "plot_model(m, show_shapes=True, to_file='model_segnet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
