{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gdal\n",
    "import os\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from keras import backend as K \n",
    "from Models import *\n",
    "from Models.utils import *\n",
    "#coding=utf-8\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import numpy as np \n",
    "from keras import *\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from keras.preprocessing.image import img_to_array  \n",
    "from keras.callbacks import ModelCheckpoint ,TensorBoard\n",
    "from SegNet0 import *\n",
    "from SegNet import *\n",
    "from FCN32 import *\n",
    "from Models.utils import *\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "import cv2\n",
    "import random\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm  \n",
    "from keras import backend as K \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "import gdal\n",
    "seed = 7  \n",
    "np.random.seed(seed)  \n",
    "# data for training  \n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network model...\n",
      "Build enceder done..\n",
      "Tensor(\"max_unpooling2d_1/max_unpooling2d_1/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_2/max_unpooling2d_2/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_3/max_unpooling2d_3/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_4/max_unpooling2d_4/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Tensor(\"max_unpooling2d_5/max_unpooling2d_5/Size_1:0\", shape=(), dtype=int32) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[INFO] model loaded\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(stride=256):\n",
    "    method = {\n",
    "              'FCN32':FCN32,\n",
    "              'SegNet0': SegNet0,\n",
    "              'SegNet': SegNet}\n",
    "    key='SegNet'\n",
    "    # load the trained convolutional neural network\n",
    "    print(\"[INFO] loading network model...\")\n",
    "    try:\n",
    "        model = load_model('D:\\Python\\seg-data/model/%s_model.h5' % key)\n",
    "    except:\n",
    "        model = method[key]() # 有自定义层时，不能直接加载模型\n",
    "        model.load_weights('D:\\Python\\seg-data/model/%s_model.h5' % key)\n",
    "    print('[INFO] model loaded')\n",
    "    image_size=stride\n",
    "    TEST_SET=['test.tif']\n",
    "    predir=r'D:\\Python\\seg-data\\data_MB/'\n",
    "    for n in range(len(TEST_SET)):\n",
    "        tif_img = gdal.Open(predir+TEST_SET[n])\n",
    "        tif_w = tif_img.RasterXSize #栅格矩阵的列数\n",
    "        tif_h = tif_img.RasterYSize\n",
    "        tif_data=tif_img.ReadAsArray(0,0,tif_w,tif_h)\n",
    "        tif_d=tif_data.shape[0]\n",
    "        tif_data=np.array(tif_data, dtype=float)\n",
    "        image=cv2.merge(tif_data)\n",
    "        #print(np.sum(tif_data[0]))\n",
    "        #plt.imshow(tif_data[0])\n",
    "        h,w,_ = image.shape\n",
    "        padding_h = (h//stride + 1) * stride \n",
    "        padding_w = (w//stride + 1) * stride\n",
    "        padding_img = np.zeros((padding_h,padding_w,_))\n",
    "        padding_img[0:h,0:w,:] = image[:,:,:]\n",
    "        b1,b2,b3,b4=cv2.split(padding_img) \n",
    "        #print(b1/np.max(b1))\n",
    "        mask_whole = np.zeros((padding_h,padding_w))\n",
    "        for i in range(padding_h//stride):\n",
    "            for j in range(padding_w//stride):\n",
    "                crop = padding_img[i*stride:i*stride+image_size,j*stride:j*stride+image_size,:]\n",
    "                ch,cw,_ = crop.shape\n",
    "                print(crop.shape)\n",
    "                if (ch != 256 or cw != 256):\n",
    "                    print ('invalid size!')\n",
    "                    continue\n",
    "                crop = np.expand_dims(crop, axis=0)\n",
    "                try:\n",
    "                    pred = model.predict_classes(crop,verbose=0)\n",
    "                    pred_prob = model.predict_proba(crop,verbose=1)\n",
    "                    print ('trying\\n')\n",
    "                except AttributeError as e:\n",
    "                    #print (crop.shape,np.sum(crop),'\\n')\n",
    "                    pred = model.predict(crop)\n",
    "                    #print((pred+0.5 ).astype(np.int32))\n",
    "                    pred=np.argmax(pred,axis=2).astype(np.float) \n",
    "                    #print(pred)\n",
    "                \n",
    "                pred = pred.reshape((256,256))\n",
    "                mask_whole[i*stride:i*stride+image_size,j*stride:j*stride+image_size] = pred[:,:]\n",
    "    plt.imshow(mask_whole,cmap='gray')\n",
    "predict(stride=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
